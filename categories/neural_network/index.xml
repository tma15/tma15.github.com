<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>neural_network on Now is better than never.</title>
    <link>https://tma15.github.io/categories/neural_network/</link>
    <description>Recent content in neural_network on Now is better than never.</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>ja</language>
    <lastBuildDate>Fri, 17 Sep 2021 16:52:50 +0900</lastBuildDate>
    
	<atom:link href="https://tma15.github.io/categories/neural_network/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>【Deep Learning】BERT学習時にbiasやlayer normalizationをweight decayしない理由</title>
      <link>https://tma15.github.io/blog/2021/09/17/deep-learningbert%E5%AD%A6%E7%BF%92%E6%99%82%E3%81%ABbias%E3%82%84layer-normalization%E3%82%92weight-decay%E3%81%97%E3%81%AA%E3%81%84%E7%90%86%E7%94%B1/</link>
      <pubDate>Fri, 17 Sep 2021 16:52:50 +0900</pubDate>
      
      <guid>https://tma15.github.io/blog/2021/09/17/deep-learningbert%E5%AD%A6%E7%BF%92%E6%99%82%E3%81%ABbias%E3%82%84layer-normalization%E3%82%92weight-decay%E3%81%97%E3%81%AA%E3%81%84%E7%90%86%E7%94%B1/</guid>
      <description>&lt;p&gt;BERTの学習で用いるoptimizerでbiasや&lt;a href=&#34;https://arxiv.org/abs/1607.06450&#34;&gt;layer normalization&lt;/a&gt;のパラメータだけがweight decayの対象外となっていることについて疑問は持ったことはあるでしょうか。たとえばhuggingfaceのtransformersの&lt;a href=&#34;https://github.com/huggingface/transformers/issues/492&#34;&gt;issue&lt;/a&gt;でもそのような質問がありますが、「Googleの公開しているBERTが&lt;a href=&#34;https://github.com/google-research/bert/blob/master/optimization.py#L65&#34;&gt;そうしている&lt;/a&gt;から再現性のために合わせた」と回答されています。ではなぜGoogleのBERT実装はそのような設定にしたのでしょうか。これらのOSSを利用されている方にも天下り的に設定している方もいらっしゃると思います。本記事ではBERTなどの学習で用いられるoptimizerのweight decayで、biasやlayer normalizationのパラメータが対象外となっている理由について解説します。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>【自然言語処理】高速なニューラル機械翻訳実装CTranslate2【論文紹介】</title>
      <link>https://tma15.github.io/blog/2020/12/13/%E8%87%AA%E7%84%B6%E8%A8%80%E8%AA%9E%E5%87%A6%E7%90%86%E9%AB%98%E9%80%9F%E3%81%AA%E3%83%8B%E3%83%A5%E3%83%BC%E3%83%A9%E3%83%AB%E6%A9%9F%E6%A2%B0%E7%BF%BB%E8%A8%B3%E5%AE%9F%E8%A3%85ctranslate2%E8%AB%96%E6%96%87%E7%B4%B9%E4%BB%8B/</link>
      <pubDate>Sun, 13 Dec 2020 17:11:32 +0900</pubDate>
      
      <guid>https://tma15.github.io/blog/2020/12/13/%E8%87%AA%E7%84%B6%E8%A8%80%E8%AA%9E%E5%87%A6%E7%90%86%E9%AB%98%E9%80%9F%E3%81%AA%E3%83%8B%E3%83%A5%E3%83%BC%E3%83%A9%E3%83%AB%E6%A9%9F%E6%A2%B0%E7%BF%BB%E8%A8%B3%E5%AE%9F%E8%A3%85ctranslate2%E8%AB%96%E6%96%87%E7%B4%B9%E4%BB%8B/</guid>
      <description>&lt;p&gt;本記事では&lt;a href=&#34;https://sites.google.com/view/wngt20/home&#34;&gt;WNGT 2020&lt;/a&gt;のefficiencyシェアドタスクに提出された&lt;a href=&#34;https://www.aclweb.org/anthology/2020.ngt-1.25.pdf&#34;&gt;Efficient and High-Quality Neural Machine Translation with OpenNMT&lt;/a&gt;を紹介します。
このタスクでは精度だけではなく、省メモリ、高速であることに焦点を当てています。
自然言語処理タスクの多くはニューラルネットワークに基づく巨大なモデルによって最高精度が塗り替えられていますが、実用上は精度以外にもメモリや速度の観点を検討しなければならない場面が多く、現実に即したタスクとなっています。
紹介する論文では機械翻訳で実験を行っていますが、その他のタスクに対しても適用できそうなテクニックが多く、勉強になりそうだったので紹介することにしました。
このタスクに参加した他のシステムも精度や速度などの指標においてパレート曲線状にあり、それぞれのシステムが重きをおいた指標が異なっています。
本記事で紹介する論文は速度、省メモリに焦点を当てています。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>【自然言語処理】 あなたのBERTに対するfine-tuningはなぜ失敗するのか 【論文紹介】</title>
      <link>https://tma15.github.io/blog/2020/10/03/%E8%87%AA%E7%84%B6%E8%A8%80%E8%AA%9E%E5%87%A6%E7%90%86-%E3%81%82%E3%81%AA%E3%81%9F%E3%81%AEbert%E3%81%AB%E5%AF%BE%E3%81%99%E3%82%8Bfine-tuning%E3%81%AF%E3%81%AA%E3%81%9C%E5%A4%B1%E6%95%97%E3%81%99%E3%82%8B%E3%81%AE%E3%81%8B-%E8%AB%96%E6%96%87%E7%B4%B9%E4%BB%8B/</link>
      <pubDate>Sat, 03 Oct 2020 09:51:17 +0900</pubDate>
      
      <guid>https://tma15.github.io/blog/2020/10/03/%E8%87%AA%E7%84%B6%E8%A8%80%E8%AA%9E%E5%87%A6%E7%90%86-%E3%81%82%E3%81%AA%E3%81%9F%E3%81%AEbert%E3%81%AB%E5%AF%BE%E3%81%99%E3%82%8Bfine-tuning%E3%81%AF%E3%81%AA%E3%81%9C%E5%A4%B1%E6%95%97%E3%81%99%E3%82%8B%E3%81%AE%E3%81%8B-%E8%AB%96%E6%96%87%E7%B4%B9%E4%BB%8B/</guid>
      <description>&lt;p&gt;本記事では&lt;a href=&#34;https://arxiv.org/abs/2006.04884&#34;&gt;On the Stability of Fine-tuning BERT: Misconceptions, Explanations, and Strong Baselines&lt;/a&gt;という論文を紹介します。
この論文ではBERTのfine-tuningが安定しにくいという問題に対して、単純で良い結果が得られる方法を提案しています。
またBERTのfine-tuningが安定しにくいという問題を細かく分析しており、参考になったのでそのあたりについてもまとめます。
本記事を読むことでBERTを自分の問題でfine-tuningするときの施策を立てやすくなるかと思います。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>【自然言語処理】LSTMに基づく文書分類 (PyTorchコード付き)</title>
      <link>https://tma15.github.io/blog/2020/09/06/%E8%87%AA%E7%84%B6%E8%A8%80%E8%AA%9E%E5%87%A6%E7%90%86lstm%E3%81%AB%E5%9F%BA%E3%81%A5%E3%81%8F%E6%96%87%E6%9B%B8%E5%88%86%E9%A1%9E-pytorch%E3%82%B3%E3%83%BC%E3%83%89%E4%BB%98%E3%81%8D/</link>
      <pubDate>Sun, 06 Sep 2020 09:46:04 +0900</pubDate>
      
      <guid>https://tma15.github.io/blog/2020/09/06/%E8%87%AA%E7%84%B6%E8%A8%80%E8%AA%9E%E5%87%A6%E7%90%86lstm%E3%81%AB%E5%9F%BA%E3%81%A5%E3%81%8F%E6%96%87%E6%9B%B8%E5%88%86%E9%A1%9E-pytorch%E3%82%B3%E3%83%BC%E3%83%89%E4%BB%98%E3%81%8D/</guid>
      <description>&lt;p&gt;本記事では日本語を対象としたLSTMに基づく文書分類モデルをPyTorchコード付きで紹介します。
以前、LSTMを用いた言語モデルについて紹介しました (
&lt;a href=&#34;https://tma15.github.io/blog/2020/03/15/%E8%87%AA%E7%84%B6%E8%A8%80%E8%AA%9E%E5%87%A6%E7%90%86-lstm%E3%81%AB%E5%9F%BA%E3%81%A5%E3%81%8F%E8%A8%80%E8%AA%9E%E3%83%A2%E3%83%87%E3%83%AB%E3%81%AE%E5%AD%A6%E7%BF%92-pytorch%E3%82%B3%E3%83%BC%E3%83%89%E4%BB%98%E3%81%8D/&#34;&gt;[自然言語処理] LSTMに基づく言語モデルの学習 (PyTorchコード付き)&lt;/a&gt;
)
が、ニューラルネットワークを用いた自然言語処理の応用例として文書分類のほうがイメージしやすそうなので、こちらについても紹介したいと思います。
実験にはライブドアコーパスから作成した、記事の見出しに対して9つのカテゴリのうち、どれか1つが付与されたデータを使います。
本記事を読むことで日本語を対象に、ニューラルネットワークを活用した自然言語処理の概要を知ることができます。
また、PyTorchで事前学習済みの単語分散表現を扱う方法も紹介しています。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>【NVIDIA直伝】あなたのPyTorchプログラムを高速化するかもしれないTips</title>
      <link>https://tma15.github.io/blog/2020/08/29/nvidia%E7%9B%B4%E4%BC%9D%E3%81%82%E3%81%AA%E3%81%9F%E3%81%AEpytorch%E3%83%97%E3%83%AD%E3%82%B0%E3%83%A9%E3%83%A0%E3%82%92%E9%AB%98%E9%80%9F%E5%8C%96%E3%81%99%E3%82%8B%E3%81%8B%E3%82%82%E3%81%97%E3%82%8C%E3%81%AA%E3%81%84tips/</link>
      <pubDate>Sat, 29 Aug 2020 13:16:11 +0900</pubDate>
      
      <guid>https://tma15.github.io/blog/2020/08/29/nvidia%E7%9B%B4%E4%BC%9D%E3%81%82%E3%81%AA%E3%81%9F%E3%81%AEpytorch%E3%83%97%E3%83%AD%E3%82%B0%E3%83%A9%E3%83%A0%E3%82%92%E9%AB%98%E9%80%9F%E5%8C%96%E3%81%99%E3%82%8B%E3%81%8B%E3%82%82%E3%81%97%E3%82%8C%E3%81%AA%E3%81%84tips/</guid>
      <description>&lt;p&gt;本記事では画像認識系の国際会議 &lt;a href=&#34;https://eccv2020.eu/&#34;&gt;ECCV2020&lt;/a&gt;のチュートリアルでNVIDIAが発表した資料 &lt;a href=&#34;https://nvlabs.github.io/eccv2020-mixed-precision-tutorial/files/szymon_migacz-pytorch-performance-tuning-guide.pdf&#34;&gt;PYTORCH PERFORMANCE TUNING GUIDE&lt;/a&gt; の内容をまとめるとともに、理解の助けになるような関連情報を参照します。
PyTorchは簡単にニューラルネットワークの実装のを容易さだけでなく、処理速度にも注意を払って開発が進められています。
プログラムにほんの少し修正を加えるだけで高速化できる可能性があります。
本記事を読み、実践することで、手元のPyTorchプログラム (特にGPUを使った学習処理) を高速化できる可能性があります。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>【自然言語処理】Scheduled samplingによるニューラル言語モデルの学習</title>
      <link>https://tma15.github.io/blog/2020/07/19/%E8%87%AA%E7%84%B6%E8%A8%80%E8%AA%9E%E5%87%A6%E7%90%86scheduled-sampling%E3%81%AB%E3%82%88%E3%82%8B%E3%83%8B%E3%83%A5%E3%83%BC%E3%83%A9%E3%83%AB%E8%A8%80%E8%AA%9E%E3%83%A2%E3%83%87%E3%83%AB%E3%81%AE%E5%AD%A6%E7%BF%92/</link>
      <pubDate>Sun, 19 Jul 2020 13:34:44 +0900</pubDate>
      
      <guid>https://tma15.github.io/blog/2020/07/19/%E8%87%AA%E7%84%B6%E8%A8%80%E8%AA%9E%E5%87%A6%E7%90%86scheduled-sampling%E3%81%AB%E3%82%88%E3%82%8B%E3%83%8B%E3%83%A5%E3%83%BC%E3%83%A9%E3%83%AB%E8%A8%80%E8%AA%9E%E3%83%A2%E3%83%87%E3%83%AB%E3%81%AE%E5%AD%A6%E7%BF%92/</guid>
      <description>&lt;p&gt;ニューラル言語モデルはこれまでのn-gram言語モデルと比較して流暢なテキストを生成することができます。
ニューラル言語モデルの学習にはTeacher-forcingという方法がよく用いられます。
この手法はニューラル言語モデルの学習がしやすい一方で、テキスト生成時の挙動と乖離があります。
本記事では、Teacher-forcingを説明するとともに、この手法の課題を改善するための手法であるScheduled samplingを紹介します。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>【自然言語処理】Kaggleコンペで利用されている文書分類のtips</title>
      <link>https://tma15.github.io/blog/2020/05/03/%E8%87%AA%E7%84%B6%E8%A8%80%E8%AA%9E%E5%87%A6%E7%90%86kaggle%E3%82%B3%E3%83%B3%E3%83%9A%E3%81%A7%E5%88%A9%E7%94%A8%E3%81%95%E3%82%8C%E3%81%A6%E3%81%84%E3%82%8B%E6%96%87%E6%9B%B8%E5%88%86%E9%A1%9E%E3%81%AEtips/</link>
      <pubDate>Sun, 03 May 2020 16:47:46 +0900</pubDate>
      
      <guid>https://tma15.github.io/blog/2020/05/03/%E8%87%AA%E7%84%B6%E8%A8%80%E8%AA%9E%E5%87%A6%E7%90%86kaggle%E3%82%B3%E3%83%B3%E3%83%9A%E3%81%A7%E5%88%A9%E7%94%A8%E3%81%95%E3%82%8C%E3%81%A6%E3%81%84%E3%82%8B%E6%96%87%E6%9B%B8%E5%88%86%E9%A1%9E%E3%81%AEtips/</guid>
      <description>&lt;p&gt;Kaggleの文書分類タスクにおける参加者のtipsが&lt;a href=&#34;https://neptune.ai/blog/text-classification-tips-and-tricks-kaggle-competitions&#34;&gt;Text Classification: All Tips and Tricks from 5 Kaggle Competitions&lt;/a&gt;にまとまっていました。英語が前提になっているものの、参考になったので目を通し、概要をまとめました。
また日本語を対象とした場合に参考になりそうな記事も挙げておきます。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>[自然言語処理] LSTMに基づく言語モデルの学習 (PyTorchコード付き)</title>
      <link>https://tma15.github.io/blog/2020/03/15/%E8%87%AA%E7%84%B6%E8%A8%80%E8%AA%9E%E5%87%A6%E7%90%86-lstm%E3%81%AB%E5%9F%BA%E3%81%A5%E3%81%8F%E8%A8%80%E8%AA%9E%E3%83%A2%E3%83%87%E3%83%AB%E3%81%AE%E5%AD%A6%E7%BF%92-pytorch%E3%82%B3%E3%83%BC%E3%83%89%E4%BB%98%E3%81%8D/</link>
      <pubDate>Sun, 15 Mar 2020 15:24:03 +0900</pubDate>
      
      <guid>https://tma15.github.io/blog/2020/03/15/%E8%87%AA%E7%84%B6%E8%A8%80%E8%AA%9E%E5%87%A6%E7%90%86-lstm%E3%81%AB%E5%9F%BA%E3%81%A5%E3%81%8F%E8%A8%80%E8%AA%9E%E3%83%A2%E3%83%87%E3%83%AB%E3%81%AE%E5%AD%A6%E7%BF%92-pytorch%E3%82%B3%E3%83%BC%E3%83%89%E4%BB%98%E3%81%8D/</guid>
      <description>&lt;p&gt;単語の系列 (たとえば文や文書) に対して確率を割り当てるようなモデルは言語モデルと呼ばれています。
古くはN-gram言語モデルが用いられました。
最近ではより広い文脈を考慮したり、単語スパースネスの問題に対処できるニューラルネットワークに基づく言語モデル (ニューラル言語モデル) が良く用いられます。
ニューラル言語モデルは文書分類、情報抽出、機械翻訳などの自然言語処理の様々なタスクで用いられます。
本記事ではコード付きでLSTMに基づく言語モデルおよびその学習方法を説明します。
本記事を読むことで、LSTMに基づく言語モデルの概要、学習の流れを理解できます。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>ニューラルネットの出力ベクトルを二値化して検索を高速化させる方法</title>
      <link>https://tma15.github.io/blog/2019/09/04/%E3%83%8B%E3%83%A5%E3%83%BC%E3%83%A9%E3%83%AB%E3%83%8D%E3%83%83%E3%83%88%E3%81%AE%E5%87%BA%E5%8A%9B%E3%83%99%E3%82%AF%E3%83%88%E3%83%AB%E3%82%92%E4%BA%8C%E5%80%A4%E5%8C%96%E3%81%97%E3%81%A6%E6%A4%9C%E7%B4%A2%E3%82%92%E9%AB%98%E9%80%9F%E5%8C%96%E3%81%95%E3%81%9B%E3%82%8B%E6%96%B9%E6%B3%95/</link>
      <pubDate>Wed, 04 Sep 2019 18:19:54 +0900</pubDate>
      
      <guid>https://tma15.github.io/blog/2019/09/04/%E3%83%8B%E3%83%A5%E3%83%BC%E3%83%A9%E3%83%AB%E3%83%8D%E3%83%83%E3%83%88%E3%81%AE%E5%87%BA%E5%8A%9B%E3%83%99%E3%82%AF%E3%83%88%E3%83%AB%E3%82%92%E4%BA%8C%E5%80%A4%E5%8C%96%E3%81%97%E3%81%A6%E6%A4%9C%E7%B4%A2%E3%82%92%E9%AB%98%E9%80%9F%E5%8C%96%E3%81%95%E3%81%9B%E3%82%8B%E6%96%B9%E6%B3%95/</guid>
      <description>&lt;p&gt;自然言語処理において、ニューラルネットワークは文や単語を実数値の密ベクトル表現に変換し、
得られた表現に基づいて目的のタスクを解くというアプローチが多い。
自然言語処理のさまざまなタスクで高い精度を上げている一方で、
テキスト検索などの高速な処理速度を要求されるような場面では密ベクトルを処理するのは
速度が遅いなどの実用的な課題がある。
自然言語処理に関する国際会議ACL 2019で発表された論文
&amp;lsquo;&amp;lsquo;Learning Compressed Sentence Representations for On-Device Text Processing&amp;rsquo;&amp;rsquo;
(&lt;a href=&#34;https://www.aclweb.org/anthology/P19-1011&#34;&gt;pdf&lt;/a&gt;)
が、類似文検索タスクにおいて、検索精度をほぼ落とさずに、高速な検索がおこなえるように、文の表現を実数値ではなく、
&lt;strong&gt;二値&lt;/strong&gt;ベクトルで表現する方法を提案した。
本記事ではこの論文でどういった技術が提案されているのかをまとめる。&lt;/p&gt;</description>
    </item>
    
  </channel>
</rss>