<!DOCTYPE html>
<html lang="ja">
  <head>
    
    <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="generator" content="Hugo 0.67.0 with theme Tranquilpeak 0.4.7-BETA">
<meta name="author" content="Takuya Makino">
<meta name="keywords" content="nlp, neural_network, paper">
<meta name="description" content="本記事ではOn the Stability of Fine-tuning BERT: Misconceptions, Explanations, and Strong Baselinesという論文を紹介します。
この論文ではBERTのfine-tuningが安定しにくいという問題に対して、単純で良い結果が得られる方法を提案しています。
またBERTのfine-tuningが安定しにくいという問題を細かく分析しており、参考になったのでそのあたりについてもまとめます。
本記事を読むことでBERTを自分の問題でfine-tuningするときの施策を立てやすくなるかと思います。">


<meta property="og:description" content="本記事ではOn the Stability of Fine-tuning BERT: Misconceptions, Explanations, and Strong Baselinesという論文を紹介します。
この論文ではBERTのfine-tuningが安定しにくいという問題に対して、単純で良い結果が得られる方法を提案しています。
またBERTのfine-tuningが安定しにくいという問題を細かく分析しており、参考になったのでそのあたりについてもまとめます。
本記事を読むことでBERTを自分の問題でfine-tuningするときの施策を立てやすくなるかと思います。">
<meta property="og:type" content="article">
<meta property="og:title" content="【自然言語処理】 あなたのBERTに対するfine-tuningはなぜ失敗するのか 【論文紹介】">
<meta name="twitter:title" content="【自然言語処理】 あなたのBERTに対するfine-tuningはなぜ失敗するのか 【論文紹介】">
<meta property="og:url" content="https://tma15.github.io/blog/2020/10/03/%E8%87%AA%E7%84%B6%E8%A8%80%E8%AA%9E%E5%87%A6%E7%90%86-%E3%81%82%E3%81%AA%E3%81%9F%E3%81%AEbert%E3%81%AB%E5%AF%BE%E3%81%99%E3%82%8Bfine-tuning%E3%81%AF%E3%81%AA%E3%81%9C%E5%A4%B1%E6%95%97%E3%81%99%E3%82%8B%E3%81%AE%E3%81%8B-%E8%AB%96%E6%96%87%E7%B4%B9%E4%BB%8B/">
<meta property="twitter:url" content="https://tma15.github.io/blog/2020/10/03/%E8%87%AA%E7%84%B6%E8%A8%80%E8%AA%9E%E5%87%A6%E7%90%86-%E3%81%82%E3%81%AA%E3%81%9F%E3%81%AEbert%E3%81%AB%E5%AF%BE%E3%81%99%E3%82%8Bfine-tuning%E3%81%AF%E3%81%AA%E3%81%9C%E5%A4%B1%E6%95%97%E3%81%99%E3%82%8B%E3%81%AE%E3%81%8B-%E8%AB%96%E6%96%87%E7%B4%B9%E4%BB%8B/">
<meta property="og:site_name" content="Now is better than never.">
<meta property="og:description" content="本記事ではOn the Stability of Fine-tuning BERT: Misconceptions, Explanations, and Strong Baselinesという論文を紹介します。
この論文ではBERTのfine-tuningが安定しにくいという問題に対して、単純で良い結果が得られる方法を提案しています。
またBERTのfine-tuningが安定しにくいという問題を細かく分析しており、参考になったのでそのあたりについてもまとめます。
本記事を読むことでBERTを自分の問題でfine-tuningするときの施策を立てやすくなるかと思います。">
<meta name="twitter:description" content="本記事ではOn the Stability of Fine-tuning BERT: Misconceptions, Explanations, and Strong Baselinesという論文を紹介します。
この論文ではBERTのfine-tuningが安定しにくいという問題に対して、単純で良い結果が得られる方法を提案しています。
またBERTのfine-tuningが安定しにくいという問題を細かく分析しており、参考になったのでそのあたりについてもまとめます。
本記事を読むことでBERTを自分の問題でfine-tuningするときの施策を立てやすくなるかと思います。">
<meta property="og:locale" content="ja">

  
    <meta property="article:published_time" content="2020-10-03T09:51:17">
  
  
    <meta property="article:modified_time" content="2020-10-03T09:51:17">
  
  
  
    
      <meta property="article:section" content="nlp">
    
      <meta property="article:section" content="neural_network">
    
      <meta property="article:section" content="paper">
    
  
  
    
      <meta property="article:tag" content="nlp">
    
      <meta property="article:tag" content="neural_network">
    
      <meta property="article:tag" content="paper">
    
  


<meta name="twitter:card" content="summary">

  <meta name="twitter:site" content="@tma15">


  <meta name="twitter:creator" content="@tma15">






  <meta property="og:image" content="https://tma15.github.io/img/2020/10/03/bert.png">
  <meta property="twitter:image" content="https://tma15.github.io/img/2020/10/03/bert.png">





  <meta property="og:image" content="https://www.gravatar.com/avatar/dfef3d85434946d893b00f14fa3b80ed?s=640">
  <meta property="twitter:image" content="https://www.gravatar.com/avatar/dfef3d85434946d893b00f14fa3b80ed?s=640">


    <title>【自然言語処理】 あなたのBERTに対するfine-tuningはなぜ失敗するのか 【論文紹介】</title>

    <link rel="icon" href="https://tma15.github.io/favicon.png">
    

    

    <link rel="canonical" href="https://tma15.github.io/blog/2020/10/03/%E8%87%AA%E7%84%B6%E8%A8%80%E8%AA%9E%E5%87%A6%E7%90%86-%E3%81%82%E3%81%AA%E3%81%9F%E3%81%AEbert%E3%81%AB%E5%AF%BE%E3%81%99%E3%82%8Bfine-tuning%E3%81%AF%E3%81%AA%E3%81%9C%E5%A4%B1%E6%95%97%E3%81%99%E3%82%8B%E3%81%AE%E3%81%8B-%E8%AB%96%E6%96%87%E7%B4%B9%E4%BB%8B/">

    
    <script type="text/javascript" async
          src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML">
    </script>
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
          tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
          });
    </script>


    <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
    <script>
          (adsbygoogle = window.adsbygoogle || []).push({
                  google_ad_client: "ca-pub-5663917297524414",
                  enable_page_level_ads: true
                });
    </script>
    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css" integrity="sha256-eZrrJcwDc/3uDhsdt61sL2oOBY362qM3lon1gyExkL0=" crossorigin="anonymous" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/2.1.4/jquery.fancybox.min.css" integrity="sha256-vuXZ9LGmmwtjqFX1F+EKin1ThZMub58gKULUyf0qECk=" crossorigin="anonymous" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/2.1.4/helpers/jquery.fancybox-thumbs.min.css" integrity="sha256-SEa4XYAHihTcEP1f5gARTB2K26Uk8PsndQYHQC1f4jU=" crossorigin="anonymous" />
    
    
    <link rel="stylesheet" href="https://tma15.github.io/css/style-twzjdbqhmnnacqs0pwwdzcdbt8yhv8giawvjqjmyfoqnvazl0dalmnhdkvp7.min.css" />
    
    
      
        <link rel="stylesheet"  href="https://tma15.github.io/css/mystyle.css">
      
    

    
      
<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
	(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
	m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
	})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
	ga('create', 'UA-20414370-4', 'auto');
	
	ga('send', 'pageview');
}
</script>

    
    
  </head>

  <body>
    <div id="blog">
      <header id="header" data-behavior="1">
  <i id="btn-open-sidebar" class="fa fa-lg fa-bars"></i>
  <div class="header-title">
    <a class="header-title-link" href="https://tma15.github.io/">Now is better than never.</a>
  </div>
  
    
      <a class="header-right-picture "
         href="https://tma15.github.io/#about">
    
    
    
      
        <img class="header-picture" src="https://www.gravatar.com/avatar/dfef3d85434946d893b00f14fa3b80ed?s=90" alt="プロフィール画像" />
      
    
    </a>
  

  <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
           (adsbygoogle = window.adsbygoogle || []).push({
                         google_ad_client: "ca-pub-5663917297524414",
                         enable_page_level_ads: true
                    });
  </script>
</header>

      <nav id="sidebar" data-behavior="1">
  <div class="sidebar-container">
    
      <div class="sidebar-profile">
        <a href="https://tma15.github.io/#about">
          <img class="sidebar-profile-picture" src="https://www.gravatar.com/avatar/dfef3d85434946d893b00f14fa3b80ed?s=110" alt="プロフィール画像" />
        </a>
        <h4 class="sidebar-profile-name">Takuya Makino</h4>
        
          <h5 class="sidebar-profile-bio">自然言語処理の研究開発に従事しています。自然言語処理に関する研究から製品化に向けた開発に興味を持っています。本ブログでは自然言語処理、機械学習、Python、C++に関する記事が多めです。詳細は<a href="https://tma15.github.io/about/">プロフィール</a>を御覧ください。</h5>
        
      </div>
    
    <ul class="sidebar-buttons">
      
  <li class="sidebar-button">
    
      <a class="sidebar-button-link " href="https://tma15.github.io/">
    
      <i class="sidebar-button-icon fa fa-lg fa-home"></i>
      
      <span class="sidebar-button-desc">ホーム</span>
    </a>
  </li>

  <li class="sidebar-button">
    
      <a class="sidebar-button-link " href="https://tma15.github.io/tags">
    
      <i class="sidebar-button-icon fa fa-lg fa-tags"></i>
      
      <span class="sidebar-button-desc">タグ</span>
    </a>
  </li>

  <li class="sidebar-button">
    
      <a class="sidebar-button-link " href="https://tma15.github.io/archives">
    
      <i class="sidebar-button-icon fa fa-lg fa-archive"></i>
      
      <span class="sidebar-button-desc">アーカイブ</span>
    </a>
  </li>

  <li class="sidebar-button">
    
      <a class="sidebar-button-link " href="https://tma15.github.io/about">
    
      <i class="sidebar-button-icon fa fa-lg fa-question"></i>
      
      <span class="sidebar-button-desc">プロフィール</span>
    </a>
  </li>


    </ul>
    <ul class="sidebar-buttons">
      
  <li class="sidebar-button">
    
      <a class="sidebar-button-link " href="https://tma15.github.io/inquiry">
    
      
      
      <span class="sidebar-button-desc">お問い合わせ</span>
    </a>
  </li>

  <li class="sidebar-button">
    
      <a class="sidebar-button-link " href="https://tma15.github.io/privacy-policy">
    
      
      
      <span class="sidebar-button-desc">プライバシーポリシー</span>
    </a>
  </li>


    </ul>
    <ul class="sidebar-buttons">
      
  <li class="sidebar-button">
    
      <a class="sidebar-button-link " href="https://tma15.github.io/index.xml">
    
      <i class="sidebar-button-icon fa fa-lg fa-rss"></i>
      
      <span class="sidebar-button-desc">RSS</span>
    </a>
  </li>


    </ul>
  
    <br>

    <h3 style="color:white">最近の投稿</h3>
    <ul >
    
    <li ><a href="https://tma15.github.io/blog/2021/09/17/deep-learningbert%E5%AD%A6%E7%BF%92%E6%99%82%E3%81%ABbias%E3%82%84layer-normalization%E3%82%92weight-decay%E3%81%97%E3%81%AA%E3%81%84%E7%90%86%E7%94%B1/" class="sidebar-button-link" style="color:white; list-style:none;">【Deep Learning】BERT学習時にbiasやlayer normalizationをweight decayしない理由</a></li>
    
    <li ><a href="https://tma15.github.io/blog/2020/12/13/%E8%87%AA%E7%84%B6%E8%A8%80%E8%AA%9E%E5%87%A6%E7%90%86%E9%AB%98%E9%80%9F%E3%81%AA%E3%83%8B%E3%83%A5%E3%83%BC%E3%83%A9%E3%83%AB%E6%A9%9F%E6%A2%B0%E7%BF%BB%E8%A8%B3%E5%AE%9F%E8%A3%85ctranslate2%E8%AB%96%E6%96%87%E7%B4%B9%E4%BB%8B/" class="sidebar-button-link" style="color:white; list-style:none;">【自然言語処理】高速なニューラル機械翻訳実装CTranslate2【論文紹介】</a></li>
    
    <li ><a href="https://tma15.github.io/blog/2020/12/12/%E6%9B%B8%E8%A9%95who-you-are%E3%83%95%E3%83%BC%E3%83%A6%E3%83%BC%E3%82%A2%E3%83%BC%E5%90%9B%E3%81%AE%E7%9C%9F%E3%81%AE%E8%A8%80%E8%91%89%E3%81%A8%E8%A1%8C%E5%8B%95%E3%81%93%E3%81%9D%E3%81%8C%E5%9B%B0%E9%9B%A3%E3%82%92%E7%94%9F%E3%81%8D%E6%8A%9C%E3%81%8F%E3%83%81%E3%83%BC%E3%83%A0%E3%82%92%E3%81%A4%E3%81%8F%E3%82%8B/" class="sidebar-button-link" style="color:white; list-style:none;">【書評】Who You Are（フーユーアー）君の真の言葉と行動こそが困難を生き抜くチームをつくる</a></li>
    
    <li ><a href="https://tma15.github.io/blog/2020/11/22/pythonmecab%E3%81%AEtagger%E3%82%AA%E3%83%96%E3%82%B8%E3%82%A7%E3%82%AF%E3%83%88%E3%82%92%E6%8C%81%E3%81%A4%E5%8D%98%E8%AA%9E%E5%88%86%E5%89%B2%E5%99%A8%E3%82%92pickle%E3%81%A7%E4%BF%9D%E5%AD%98%E3%81%99%E3%82%8B%E6%96%B9%E6%B3%95/" class="sidebar-button-link" style="color:white; list-style:none;">【Python】MeCabのTaggerオブジェクトを持つ単語分割器をpickleで保存する方法</a></li>
    
    <li ><a href="https://tma15.github.io/blog/2020/11/21/%E8%87%AA%E7%84%B6%E8%A8%80%E8%AA%9E%E5%87%A6%E7%90%86scikit-learn%E3%81%A7tfidf%E3%81%A8%E3%81%9D%E3%82%8C%E4%BB%A5%E5%A4%96%E3%81%AE%E7%89%B9%E5%BE%B4%E9%87%8F%E3%82%92%E7%B5%84%E3%81%BF%E5%90%88%E3%82%8F%E3%81%9B%E3%82%8B/" class="sidebar-button-link" style="color:white; list-style:none;">【自然言語処理】scikit-learnでtfidfとそれ以外の特徴量を組み合わせる</a></li>
    
    </ul>


      <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
      <ins class="adsbygoogle"
      style="display:block; text-align:center;"
      data-ad-layout="in-article"
      data-ad-format="fluid"
      data-ad-client="ca-pub-5663917297524414"
      data-ad-slot="8357823829"></ins>
      <script>
      (adsbygoogle = window.adsbygoogle || []).push({});
      </script>

  </div>
</nav>

      

      <div id="main" data-behavior="1"
        class="
               hasCoverMetaIn
               ">
        <article class="post" itemscope itemType="http://schema.org/BlogPosting">
          
          
            <div class="post-header main-content-wrap text-left">
  
    <h1 class="post-title" itemprop="headline">
      【自然言語処理】 あなたのBERTに対するfine-tuningはなぜ失敗するのか 【論文紹介】
    </h1>
  
  
  <div class="postShorten-meta post-meta">
    
      <time itemprop="datePublished" datetime="2020-10-03T09:51:17&#43;09:00">
        
  
  
  
  
    2020-10-03
  


      </time>

      

    
    
  
  
    <span>カテゴリー</span>
    
      <a class="category-link" href="https://tma15.github.io/categories/nlp">nlp</a>, 
    
      <a class="category-link" href="https://tma15.github.io/categories/neural_network">neural_network</a>, 
    
      <a class="category-link" href="https://tma15.github.io/categories/paper">paper</a>
    
  

  </div>


</div>
          

          <div class="post-content markdown" itemprop="articleBody">
            <div class="main-content-wrap">
              <p>本記事では<a href="https://arxiv.org/abs/2006.04884">On the Stability of Fine-tuning BERT: Misconceptions, Explanations, and Strong Baselines</a>という論文を紹介します。
この論文ではBERTのfine-tuningが安定しにくいという問題に対して、単純で良い結果が得られる方法を提案しています。
またBERTのfine-tuningが安定しにくいという問題を細かく分析しており、参考になったのでそのあたりについてもまとめます。
本記事を読むことでBERTを自分の問題でfine-tuningするときの施策を立てやすくなるかと思います。</p>
<h2 id="table-of-contents">目次</h2><nav id="TableOfContents">
  <ul>
    <li><a href="#fine-tuningが失敗する原因とされていること">fine-tuningが失敗する原因とされていること</a></li>
    <li><a href="#従来のfine-tuning失敗の原因に関する調査">従来のfine-tuning失敗の原因に関する調査</a>
      <ul>
        <li><a href="#検証に利用するデータ">検証に利用するデータ</a></li>
        <li><a href="#fine-tuning失敗の基準">fine-tuning失敗の基準</a></li>
        <li><a href="#破滅的忘却がfine-tuningの失敗を引き起こすのか">破滅的忘却がfine-tuningの失敗を引き起こすのか？</a></li>
        <li><a href="#学習データが少ないことがfine-tuningの失敗を引き起こすのか">学習データが少ないことがfine-tuningの失敗を引き起こすのか？</a></li>
      </ul>
    </li>
    <li><a href="#fine-tuning失敗の理由はoptimizationにある">fine-tuning失敗の理由はoptimizationにある</a>
      <ul>
        <li><a href="#optimizationに関する調査">optimizationに関する調査</a></li>
        <li><a href="#generalizationに関する調査">generalizationに関する調査</a></li>
      </ul>
    </li>
    <li><a href="#bertをfine-tuningするときに検討したいこと">BERTをfine-tuningするときに検討したいこと</a>
      <ul>
        <li><a href="#単純だけど強力なfine-tuningのコツ">単純だけど強力なfine-tuningのコツ</a></li>
        <li><a href="#実験結果">実験結果</a></li>
      </ul>
    </li>
    <li><a href="#おわり">おわり</a></li>
  </ul>
</nav>


<script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<ins class="adsbygoogle"
     style="display:block; text-align:center;"
     data-ad-layout="in-article"
     data-ad-format="fluid"
     data-ad-client="ca-pub-5663917297524414"
     data-ad-slot="8357823829"></ins>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>


<p>本記事で掲載する図や表は紹介する論文から引用しています。</p>
<p>紹介する論文で提案する方法でBERTをfine-tuningすることで、Figure 1のように学習が安定し、かつ平均的にも高い評価尺度が得られるようになります。</p>

 
  
  
  
  
    
      
    
  
    
      
    
  
    
  

 
  
  
  
  
    
  
    
      
    
  

<div class="figure center" >
  
    <a class="fancybox" href="https://tma15.github.io/img/2020/10/03/comparison-of-results.png" title="Fiure 1。既存fine-tuningの比較結果" data-fancybox-group="">
  
    <img class="fig-img" src="https://tma15.github.io/img/2020/10/03/comparison-of-results.png"  alt="Fiure 1。既存fine-tuningの比較結果">
  
    </a>
  
   
    <span class="caption">Fiure 1。既存fine-tuningの比較結果</span>
  
</div>

  <div style="clear:both;"></div>

<h2 id="fine-tuningが失敗する原因とされていること">fine-tuningが失敗する原因とされていること</h2>
<p><strong>従来fine-tuningが失敗する原因とされていること:</strong></p>
<ul>
<li>破滅的忘却</li>
<li>学習データ量が少ないこと</li>
</ul>
<p>破滅的忘却は、「ここではfine-tuningによってpre-trainingで学習したパラメータから大きく変わってしまい、pre-trainingによってできるようになっていたことができなくなってしまうような現象」です。</p>
<p>一方で、紹介する論文が主張する、<strong>fine-tuning失敗する原因はoptimizationにおける勾配消失</strong>にあるとしています。
この仮設を検証するために、BERT, RoBERTa, ALBERTといったBERT系の手法をGLUEに含まれるベンチマークデータで実証しています。</p>
<h2 id="従来のfine-tuning失敗の原因に関する調査">従来のfine-tuning失敗の原因に関する調査</h2>
<p>まずfine-tuningが失敗する原因としてよく言及される破滅的忘却と、学習データの量が少ないことに着目し、検証実験をおこないます。</p>
<h3 id="検証に利用するデータ">検証に利用するデータ</h3>
<ul>
<li>CoLA: 与えられた文が文法的かそうでないかを予測するタスク</li>
<li>MPRC: 2つの文が与えられたとき、片方がもう片方の文の言い換えであるかそうでないかを予測するタスク</li>
<li>RTE: 2つの文の含意関係を認識するタスク</li>
</ul>
<h3 id="fine-tuning失敗の基準">fine-tuning失敗の基準</h3>
<p>本記事で紹介する論文では、「fine-tuningを終えたBERTの評価データにおける評価尺度が、そのタスクのベースラインのそれよりも低い」場合、fine-tuningに失敗したBERTとしています。</p>
<h3 id="破滅的忘却がfine-tuningの失敗を引き起こすのか">破滅的忘却がfine-tuningの失敗を引き起こすのか？</h3>

 
  
  
  
  
    
      
    
  
    
      
    
  
    
  

 
  
  
  
  
    
  
    
      
    
  

<div class="figure center" >
  
    <a class="fancybox" href="https://tma15.github.io/img/2020/10/03/perplexity-and-performance-on-dev.png" title="破滅的忘却がfine-tuningの失敗に及ぼす影響の検証実験。" data-fancybox-group="">
  
    <img class="fig-img" src="https://tma15.github.io/img/2020/10/03/perplexity-and-performance-on-dev.png"  alt="破滅的忘却がfine-tuningの失敗に及ぼす影響の検証実験。">
  
    </a>
  
   
    <span class="caption">破滅的忘却がfine-tuningの失敗に及ぼす影響の検証実験。</span>
  
</div>

  <div style="clear:both;"></div>

<p>破滅的忘却は、一般的に言うと、2つの異なるタスクに対して、逐次的にニューラルネットワークを学習するとき、２つ目のタスクを学習すると、１つ目のタスクに対してうまく予測ができなくなったしまうような現象です。</p>
<p>本記事で紹介する論文では、pre-trainingをしてからfine-tuningするという2つのタスクを逐次的に処理する流れになります。
そのため、破滅的忘却が起きているのならば <strong>「fine-tuning後、pre-trainingのタスク、つまり単語の穴埋めをするような問題がうまく解けなくなっている」</strong> ことになります。</p>
<p>破滅的忘却が起きているかどうかを計測するために、BERTの上位層を事前学習時の学習結果に変えて、pre-trainingのタスクがうまく解けているかを評価します。
評価尺度にはpre-trainingデータに対するパープレキシティ (ここでは穴埋めされた箇所に対して、正解の単語の生成確率をうまく予測できているかどうかを表すような指標)を用います。</p>
<p>実験ではfine-tuningが成功したBERTと、fine-tuningが失敗したBERTのパープレキシティを比較します。
結果を見ると、上位のTransformer層 (10層目まで) をpre-training時の学習結果にすると、破滅的忘却 (パープレキシティが高い) が起きているようです (fine-tuning後の上位の層の学習結果はpre-trainingのそれと大きく変わっているということです)。
ただし、 <strong>破滅的忘却はfine-tuningの成否にかかわらず起きています。</strong></p>
<p>また、破滅的忘却は通常、少なくとも後段のタスク、つまりfine-tuningが成功していることが前提になりますが、fine-tuningが失敗したBERTは、そうではありません。
Figure 2(c)を見ると、含意関係認識タスクのfine-tuningが失敗したBERTは、学習時の損失が小さくなっているのにも関わらず、開発データにおけるaccuracyがベースラインと同程度となっており、学習はできているものの、よく汎化できていないことが示唆されます。</p>
<p>これらのことから、 <strong>fine-tuningの失敗の原因はoptimizationにあって、それがBERTの上位層における破滅的忘却を引き起こしているのではないかと推測されます。</strong></p>
<h3 id="学習データが少ないことがfine-tuningの失敗を引き起こすのか">学習データが少ないことがfine-tuningの失敗を引き起こすのか？</h3>

 
  
  
  
  
    
      
    
  
    
      
    
  
    
  

 
  
  
  
  
    
  
    
      
    
  

<div class="figure center" >
  
    <a class="fancybox" href="https://tma15.github.io/img/2020/10/03/results-on-small-training-data.png" title="Fiure 3" data-fancybox-group="">
  
    <img class="fig-img" src="https://tma15.github.io/img/2020/10/03/results-on-small-training-data.png"  alt="Fiure 3">
  
    </a>
  
   
    <span class="caption">Fiure 3</span>
  
</div>

  <div style="clear:both;"></div>

<p>次に学習データの量が少ないことがfine-tuningの失敗の原因となるかを実験に基づいて検証します。
実験では学習データを少量にサンプリングし、次の2つの設定を比較します。</p>
<ol>
<li>少量にサンプリングした学習データでfine-tuning</li>
<li>少量にサンプリングした学習データでfine-tuning。ただし、学習データをすべて使った場合と同じ数のiterationだけパラメータを更新する。</li>
</ol>
<p><strong>学習データの量が少ないことが問題であるならば、1と2で同等の結果となることが期待されます。</strong></p>
<p>しかしながら、実験結果 (Figure 3) から観測できることは以下のとおりです。</p>
<ul>
<li>学習データの量を減らすとfine-tuningの失敗に影響する</li>
<li>学習データの量を減らすと汎化性能が低下する</li>
<li>学習データの量を減らしても減らさない場合と同じだけのiteration回数だけパラメータを更新すると、学習が安定しやすくなる</li>
</ul>
<p>これらのことから、 <strong>学習データの量が少ないことは、学習の安定性とは関係がないこと、学習の安定に影響するのはiteration回数であることを示唆しています。</strong></p>
<h2 id="fine-tuning失敗の理由はoptimizationにある">fine-tuning失敗の理由はoptimizationにある</h2>
<p>破滅的忘却と学習データの量が少ないことはfine-tuningの失敗と相関があるものの、それらが原因とは言えないことを見てきました。</p>
<h3 id="optimizationに関する調査">optimizationに関する調査</h3>
<h4 id="fine-tuningを失敗したbertは勾配消失が起きている">fine-tuningを失敗したBERTは勾配消失が起きている</h4>

 
  
  
  
  
    
      
    
  
    
      
    
  
    
  

 
  
  
  
  
    
  
    
      
    
  

<div class="figure center" >
  
    <a class="fancybox" href="https://tma15.github.io/img/2020/10/03/gradient-norms.png" title="Transformer層の勾配の大きさ" data-fancybox-group="">
  
    <img class="fig-img" src="https://tma15.github.io/img/2020/10/03/gradient-norms.png"  alt="Transformer層の勾配の大きさ">
  
    </a>
  
   
    <span class="caption">Transformer層の勾配の大きさ</span>
  
</div>

  <div style="clear:both;"></div>

<p>Figure 2(c) では学習時の損失がほとんど変化していませんでした。
この現象を調査するために、fine-tuningが成功したBERTと、失敗したBERTの各層における勾配の大きさ (L2ノルム) を計測しています (Figure 4)。
Figure 4から、fine-tuningが失敗したBERTは、学習が進むと入力に近い層で勾配の大きさが小さくなっていることが確認できます。
このことから、fine-tuningが失敗したBERTは、勾配消失が起きており、パラメータがほとんど更新されず、学習の損失が変わらなかったと推測できます。</p>
<p>またfine-tuningにおける勾配消失は、通常の勾配消失よりも解決が難しいことを言及しています。
なぜなら、パラメータの初期化方法は、学習が安定して進むような研究が進められていますが、pre-trainingしたモデルのパラメータを修正してしまうと、pre-trainingで学習した結果を変えてしまうことになるので、単純な方法ではパラメータの修正ができないためです。</p>
<h4 id="オリジナルのbertのfine-tuneにおけるoptimizationはバイアス補正を利用していない">オリジナルのBERTのfine-tuneにおけるoptimizationはバイアス補正を利用していない</h4>

 
  
  
  
  
    
      
    
  
    
      
    
  
    
  

 
  
  
  
  
    
  
    
      
    
  

<div class="figure center" >
  
    <a class="fancybox" href="https://tma15.github.io/img/2020/10/03/results-on-different-lr-bc.png" title="学習率・バイアス補正の影響" data-fancybox-group="">
  
    <img class="fig-img" src="https://tma15.github.io/img/2020/10/03/results-on-different-lr-bc.png"  alt="学習率・バイアス補正の影響">
  
    </a>
  
   
    <span class="caption">学習率・バイアス補正の影響</span>
  
</div>

  <div style="clear:both;"></div>

<p>Adamのバイアス補正は、BERTの学習でよく用いられるwarm up trainingと同様の効果を持つことを言及しています。
Figure 5はバイアス補正あり・なしのAdamでfine-tuningを実施したモデルの比較結果を表しています。
BERTの<a href="https://www.aclweb.org/anthology/N19-1423.pdf">元論文</a>ではバイアス補正なしのAdamを用いています<sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>が、Figure 5から、バイアス補正ありのAdamを用いることが安定的な学習につながることを示しています。</p>
<h4 id="fine-tuningに失敗したbertは局所解に陥っている">fine-tuningに失敗したBERTは局所解に陥っている</h4>

 
  
  
  
  
    
      
    
  
    
      
    
  
    
  

 
  
  
  
  
    
  
    
      
    
  

<div class="figure center" >
  
    <a class="fancybox" href="https://tma15.github.io/img/2020/10/03/loss-surface.png" title="損失の形状。青色ほど損失の値が低い。" data-fancybox-group="">
  
    <img class="fig-img" src="https://tma15.github.io/img/2020/10/03/loss-surface.png"  alt="損失の形状。青色ほど損失の値が低い。">
  
    </a>
  
   
    <span class="caption">損失の形状。青色ほど損失の値が低い。</span>
  
</div>

  <div style="clear:both;"></div>

<p>fine-tuningの失敗を直感的に理解するために、fine-tuningに成功したBERTと失敗したBERTの損失の形状を可視化しています (Figure 7)。
fine-tuningに失敗したBERT ($\theta_f$) は局所解に陥っていること、成功したBERT ($\theta_s$) はより損失が小さな解が求められていることがわかります。
fine-tuningに失敗したBERTは局所解に陥ったあたりで勾配消失が起き、パラメータが更新されないため、そこから抜け出せなくなっているというのが視覚的にわかります。</p>
<h3 id="generalizationに関する調査">generalizationに関する調査</h3>

 
  
  
  
  
    
      
    
  
    
      
    
  
    
  

 
  
  
  
  
    
  
    
      
    
  

<div class="figure center" >
  
    <a class="fancybox" href="https://tma15.github.io/img/2020/10/03/dev-acc.png" title="iteration回数と汎化性能の関係・学習データにおける損失と汎化性能の関係" data-fancybox-group="">
  
    <img class="fig-img" src="https://tma15.github.io/img/2020/10/03/dev-acc.png" style="height: 600px;" alt="iteration回数と汎化性能の関係・学習データにおける損失と汎化性能の関係">
  
    </a>
  
   
    <span class="caption">iteration回数と汎化性能の関係・学習データにおける損失と汎化性能の関係</span>
  
</div>

  <div style="clear:both;"></div>

<p>これまでにoptimizationが学習の安定に与える影響を見てきました。
次は汎化性能と学習の安定について調査します。</p>
<p>Figure 8はBERTのデフォルトの設定でのfine-tuneを10回実施して得られた結果です。
Figure 8(a)はfine-tuningに成功したBERTの開発データにおけるaccuracyです。
Figure 8(b)はfine-tuningの成否にかかわらず、すべてのBERTの実験における学習時の損失と開発データにおけるaccuracyです。</p>
<p>Figure 8(a)から <strong>iterationを増やすと開発データにおけるaccuracyが改善されている</strong> ことがわかります。
また、Figure 8(b)から <strong>fine-tuningの成否にかかわらず、学習時の損失が小さい方が開発データにおけるaccuracyが高い</strong> ことがわかります。</p>
<p>これらのことから、 <strong>過学習はfine-tuningにおいては問題ではない</strong> ことがいえます。また、 <strong>学習のiterationを増やすことで、汎化性能が改善されている</strong> ことがいえます。</p>
<h2 id="bertをfine-tuningするときに検討したいこと">BERTをfine-tuningするときに検討したいこと</h2>
<p>ではどうすれば安定的にBERTのfine-tuningを成功されられるのでしょうか。
本記事で紹介している論文では以下を提案し、実験によって有効性を評価しています。</p>
<h3 id="単純だけど強力なfine-tuningのコツ">単純だけど強力なfine-tuningのコツ</h3>
<ul>
<li>学習時初期における勾配消失をさけるために、バイアス補正つきのoptimizationと、小さな学習率を使う</li>
<li>学習のiteration回数を増やし、early stoppingを使いつつも学習時の損失を小さくする</li>
</ul>
<table>
<thead>
<tr>
<th align="left"></th>
<th align="left">BERTのfine-tuningの設定</th>
<th align="left">紹介論文のfine-tuningの設定</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left">optimizer</td>
<td align="left">バイアス補正のないAdamW</td>
<td align="left">AdamW</td>
</tr>
<tr>
<td align="left">エポック数</td>
<td align="left">3</td>
<td align="left">20</td>
</tr>
<tr>
<td align="left">学習の終了基準</td>
<td align="left">なし</td>
<td align="left">開発データにおけるタスクの評価尺度が改善しなくなったら終了</td>
</tr>
</tbody>
</table>
<h3 id="実験結果">実験結果</h3>

 
  
  
  
  
    
      
    
  
    
      
    
  
    
  

 
  
  
  
  
    
  
    
      
    
  

<div class="figure center" >
  
    <a class="fancybox" href="https://tma15.github.io/img/2020/10/03/experimental-results.png" title="実験結果" data-fancybox-group="">
  
    <img class="fig-img" src="https://tma15.github.io/img/2020/10/03/experimental-results.png"  alt="実験結果">
  
    </a>
  
   
    <span class="caption">実験結果</span>
  
</div>

  <div style="clear:both;"></div>

<p>Table 1に実験結果が示されています。
実験結果から、上記の設定を利用することで、毎回乱数を変えて実験しても評価尺度が安定するだけではなく、平均的に見てもBERTのデフォルトの設定よりも良い結果が得られています。</p>
<p>提案手法は学習のエポック数が増えるため、計算コストが増えるという懸念事項もあります。
一方で、提案手法は学習が安定しやすいため、何度も実験を繰り返さなくてもよい結果が得られます。</p>
<h2 id="おわり">おわり</h2>
<p>本記事では<a href="https://arxiv.org/pdf/2006.04884.pdf">On the Stability of Fine-tuning BERT: Misconceptions, Explanations, and Strong Baselines</a>という論文を紹介しました。
紹介した論文ではBERTのfine-tuningが安定しにくいという問題に対して、単純で良い結果が得られる方法を提案しました。</p>
<p><a href="//af.moshimo.com/af/c/click?a_id=2804718&amp;p_id=54&amp;pc_id=54&amp;pl_id=616&amp;url=https%3A%2F%2Fitem.rakuten.co.jp%2Fbook%2F16733594%2F&amp;m=http%3A%2F%2Fm.rakuten.co.jp%2Fbook%2Fi%2F20354534%2F" rel="nofollow" referrerpolicy="no-referrer-when-downgrade"><img src="//thumbnail.image.rakuten.co.jp/@0_mall/book/cabinet/7264/9784274227264_2.jpg?_ex=128x128" alt="" style="border: none;" /><br />BERTによる自然言語処理入門 Transformersを使った実践プログラミング [ ストックマーク株式会社 ]</a><img src="//i.moshimo.com/af/i/impression?a_id=2804718&amp;p_id=54&amp;pc_id=54&amp;pl_id=616" alt="" width="1" height="1" style="border: 0px;" /></p>
<section class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1" role="doc-endnote">
<p><a href="https://github.com/huggingface/transformers/issues/420">https://github.com/huggingface/transformers/issues/420</a> <a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</section>
              

              <br>
              <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
              <ins class="adsbygoogle"
              style="display:block; text-align:center;"
              data-ad-layout="in-article"
              data-ad-format="fluid"
              data-ad-client="ca-pub-5663917297524414"
              data-ad-slot="8357823829"></ins>
              <script>
              (adsbygoogle = window.adsbygoogle || []).push({});
              </script>
    
              

<h3>関連記事</h3>

<div class="box">

  
    
  


  

<article class="postShorten postShorten--thumbnailimg-left" itemscope itemType="http://schema.org/BlogPosting">
  <div class="postShorten-wrap">
    
    <div class="postShorten-header">
      <h4 class="postShorten-title" itemprop="headline">
        <a class="link-unstyled" href="https://tma15.github.io/blog/2019/09/04/%E3%83%8B%E3%83%A5%E3%83%BC%E3%83%A9%E3%83%AB%E3%83%8D%E3%83%83%E3%83%88%E3%81%AE%E5%87%BA%E5%8A%9B%E3%83%99%E3%82%AF%E3%83%88%E3%83%AB%E3%82%92%E4%BA%8C%E5%80%A4%E5%8C%96%E3%81%97%E3%81%A6%E6%A4%9C%E7%B4%A2%E3%82%92%E9%AB%98%E9%80%9F%E5%8C%96%E3%81%95%E3%81%9B%E3%82%8B%E6%96%B9%E6%B3%95/">
          ニューラルネットの出力ベクトルを二値化して検索を高速化させる方法
        </a>
      </h4>
      
  <div class="postShorten-meta post-meta">
    
      <time itemprop="datePublished" datetime="2019-09-04T18:19:54&#43;09:00">
        
  
  
  
  
    2019-09-04
  


      </time>

      

    
    
  
  
    <span>カテゴリー</span>
    
      <a class="category-link" href="https://tma15.github.io/categories/neural_network">neural_network</a>, 
    
      <a class="category-link" href="https://tma15.github.io/categories/acl2019">acl2019</a>, 
    
      <a class="category-link" href="https://tma15.github.io/categories/nlp">nlp</a>, 
    
      <a class="category-link" href="https://tma15.github.io/categories/paper">paper</a>
    
  

  </div>


    </div>









  </div>
  
    <a href="https://tma15.github.io/blog/2019/09/04/%E3%83%8B%E3%83%A5%E3%83%BC%E3%83%A9%E3%83%AB%E3%83%8D%E3%83%83%E3%83%88%E3%81%AE%E5%87%BA%E5%8A%9B%E3%83%99%E3%82%AF%E3%83%88%E3%83%AB%E3%82%92%E4%BA%8C%E5%80%A4%E5%8C%96%E3%81%97%E3%81%A6%E6%A4%9C%E7%B4%A2%E3%82%92%E9%AB%98%E9%80%9F%E5%8C%96%E3%81%95%E3%81%9B%E3%82%8B%E6%96%B9%E6%B3%95/">
      <div class="postShorten-thumbnailimg">
        <img alt="" itemprop="image" src="https://tma15.github.io/img/2019/learning-to-compress.PNG"/>
      </div>
    </a>
  
</article>

</div>
<br>

<div class="box">

  
    
  


  

<article class="postShorten postShorten--thumbnailimg-left" itemscope itemType="http://schema.org/BlogPosting">
  <div class="postShorten-wrap">
    
    <div class="postShorten-header">
      <h4 class="postShorten-title" itemprop="headline">
        <a class="link-unstyled" href="https://tma15.github.io/blog/2020/09/06/%E8%87%AA%E7%84%B6%E8%A8%80%E8%AA%9E%E5%87%A6%E7%90%86lstm%E3%81%AB%E5%9F%BA%E3%81%A5%E3%81%8F%E6%96%87%E6%9B%B8%E5%88%86%E9%A1%9E-pytorch%E3%82%B3%E3%83%BC%E3%83%89%E4%BB%98%E3%81%8D/">
          【自然言語処理】LSTMに基づく文書分類 (PyTorchコード付き)
        </a>
      </h4>
      
  <div class="postShorten-meta post-meta">
    
      <time itemprop="datePublished" datetime="2020-09-06T09:46:04&#43;09:00">
        
  
  
  
  
    2020-09-06
  


      </time>

      

    
    
  
  
    <span>カテゴリー</span>
    
      <a class="category-link" href="https://tma15.github.io/categories/nlp">nlp</a>, 
    
      <a class="category-link" href="https://tma15.github.io/categories/python">python</a>, 
    
      <a class="category-link" href="https://tma15.github.io/categories/pytorch">pytorch</a>, 
    
      <a class="category-link" href="https://tma15.github.io/categories/neural_network">neural_network</a>, 
    
      <a class="category-link" href="https://tma15.github.io/categories/machine_learning">machine_learning</a>, 
    
      <a class="category-link" href="https://tma15.github.io/categories/google-colab">google-colab</a>
    
  

  </div>


    </div>









  </div>
  
    <a href="https://tma15.github.io/blog/2020/09/06/%E8%87%AA%E7%84%B6%E8%A8%80%E8%AA%9E%E5%87%A6%E7%90%86lstm%E3%81%AB%E5%9F%BA%E3%81%A5%E3%81%8F%E6%96%87%E6%9B%B8%E5%88%86%E9%A1%9E-pytorch%E3%82%B3%E3%83%BC%E3%83%89%E4%BB%98%E3%81%8D/">
      <div class="postShorten-thumbnailimg">
        <img alt="" itemprop="image" src="https://tma15.github.io/img/2020/09/06/document-classification.png"/>
      </div>
    </a>
  
</article>

</div>
<br>

<div class="box">

  
    
  


  

<article class="postShorten postShorten--thumbnailimg-left" itemscope itemType="http://schema.org/BlogPosting">
  <div class="postShorten-wrap">
    
    <div class="postShorten-header">
      <h4 class="postShorten-title" itemprop="headline">
        <a class="link-unstyled" href="https://tma15.github.io/blog/2020/07/19/%E8%87%AA%E7%84%B6%E8%A8%80%E8%AA%9E%E5%87%A6%E7%90%86scheduled-sampling%E3%81%AB%E3%82%88%E3%82%8B%E3%83%8B%E3%83%A5%E3%83%BC%E3%83%A9%E3%83%AB%E8%A8%80%E8%AA%9E%E3%83%A2%E3%83%87%E3%83%AB%E3%81%AE%E5%AD%A6%E7%BF%92/">
          【自然言語処理】Scheduled samplingによるニューラル言語モデルの学習
        </a>
      </h4>
      
  <div class="postShorten-meta post-meta">
    
      <time itemprop="datePublished" datetime="2020-07-19T13:34:44&#43;09:00">
        
  
  
  
  
    2020-07-19
  


      </time>

      

    
    
  
  
    <span>カテゴリー</span>
    
      <a class="category-link" href="https://tma15.github.io/categories/nlp">nlp</a>, 
    
      <a class="category-link" href="https://tma15.github.io/categories/neural_network">neural_network</a>, 
    
      <a class="category-link" href="https://tma15.github.io/categories/machine_learning">machine_learning</a>
    
  

  </div>


    </div>









  </div>
  
    <a href="https://tma15.github.io/blog/2020/07/19/%E8%87%AA%E7%84%B6%E8%A8%80%E8%AA%9E%E5%87%A6%E7%90%86scheduled-sampling%E3%81%AB%E3%82%88%E3%82%8B%E3%83%8B%E3%83%A5%E3%83%BC%E3%83%A9%E3%83%AB%E8%A8%80%E8%AA%9E%E3%83%A2%E3%83%87%E3%83%AB%E3%81%AE%E5%AD%A6%E7%BF%92/">
      <div class="postShorten-thumbnailimg">
        <img alt="" itemprop="image" src="https://tma15.github.io/img/2020/07/19/scheduled-sampling.png"/>
      </div>
    </a>
  
</article>

</div>
<br>

<div class="box">

  
    
  


  

<article class="postShorten postShorten--thumbnailimg-left" itemscope itemType="http://schema.org/BlogPosting">
  <div class="postShorten-wrap">
    
    <div class="postShorten-header">
      <h4 class="postShorten-title" itemprop="headline">
        <a class="link-unstyled" href="https://tma15.github.io/blog/2020/05/03/%E8%87%AA%E7%84%B6%E8%A8%80%E8%AA%9E%E5%87%A6%E7%90%86kaggle%E3%82%B3%E3%83%B3%E3%83%9A%E3%81%A7%E5%88%A9%E7%94%A8%E3%81%95%E3%82%8C%E3%81%A6%E3%81%84%E3%82%8B%E6%96%87%E6%9B%B8%E5%88%86%E9%A1%9E%E3%81%AEtips/">
          【自然言語処理】Kaggleコンペで利用されている文書分類のtips
        </a>
      </h4>
      
  <div class="postShorten-meta post-meta">
    
      <time itemprop="datePublished" datetime="2020-05-03T16:47:46&#43;09:00">
        
  
  
  
  
    2020-05-03
  


      </time>

      

    
    
  
  
    <span>カテゴリー</span>
    
      <a class="category-link" href="https://tma15.github.io/categories/machine_learning">machine_learning</a>, 
    
      <a class="category-link" href="https://tma15.github.io/categories/neural_network">neural_network</a>, 
    
      <a class="category-link" href="https://tma15.github.io/categories/kaggle">kaggle</a>, 
    
      <a class="category-link" href="https://tma15.github.io/categories/nlp">nlp</a>
    
  

  </div>


    </div>









  </div>
  
    <a href="https://tma15.github.io/blog/2020/05/03/%E8%87%AA%E7%84%B6%E8%A8%80%E8%AA%9E%E5%87%A6%E7%90%86kaggle%E3%82%B3%E3%83%B3%E3%83%9A%E3%81%A7%E5%88%A9%E7%94%A8%E3%81%95%E3%82%8C%E3%81%A6%E3%81%84%E3%82%8B%E6%96%87%E6%9B%B8%E5%88%86%E9%A1%9E%E3%81%AEtips/">
      <div class="postShorten-thumbnailimg">
        <img alt="" itemprop="image" src="https://tma15.github.io/img/Kaggle_logo.png"/>
      </div>
    </a>
  
</article>

</div>
<br>

<div class="box">

  
    
  


  

<article class="postShorten postShorten--thumbnailimg-left" itemscope itemType="http://schema.org/BlogPosting">
  <div class="postShorten-wrap">
    
    <div class="postShorten-header">
      <h4 class="postShorten-title" itemprop="headline">
        <a class="link-unstyled" href="https://tma15.github.io/blog/2020/03/15/%E8%87%AA%E7%84%B6%E8%A8%80%E8%AA%9E%E5%87%A6%E7%90%86-lstm%E3%81%AB%E5%9F%BA%E3%81%A5%E3%81%8F%E8%A8%80%E8%AA%9E%E3%83%A2%E3%83%87%E3%83%AB%E3%81%AE%E5%AD%A6%E7%BF%92-pytorch%E3%82%B3%E3%83%BC%E3%83%89%E4%BB%98%E3%81%8D/">
          [自然言語処理] LSTMに基づく言語モデルの学習 (PyTorchコード付き)
        </a>
      </h4>
      
  <div class="postShorten-meta post-meta">
    
      <time itemprop="datePublished" datetime="2020-03-15T15:24:03&#43;09:00">
        
  
  
  
  
    2020-03-15
  


      </time>

      

    
    
  
  
    <span>カテゴリー</span>
    
      <a class="category-link" href="https://tma15.github.io/categories/neural_network">neural_network</a>, 
    
      <a class="category-link" href="https://tma15.github.io/categories/nlp">nlp</a>, 
    
      <a class="category-link" href="https://tma15.github.io/categories/pytorch">pytorch</a>, 
    
      <a class="category-link" href="https://tma15.github.io/categories/python">python</a>, 
    
      <a class="category-link" href="https://tma15.github.io/categories/google-colab">google-colab</a>, 
    
      <a class="category-link" href="https://tma15.github.io/categories/machine_learning">machine_learning</a>
    
  

  </div>


    </div>









  </div>
  
    <a href="https://tma15.github.io/blog/2020/03/15/%E8%87%AA%E7%84%B6%E8%A8%80%E8%AA%9E%E5%87%A6%E7%90%86-lstm%E3%81%AB%E5%9F%BA%E3%81%A5%E3%81%8F%E8%A8%80%E8%AA%9E%E3%83%A2%E3%83%87%E3%83%AB%E3%81%AE%E5%AD%A6%E7%BF%92-pytorch%E3%82%B3%E3%83%BC%E3%83%89%E4%BB%98%E3%81%8D/">
      <div class="postShorten-thumbnailimg">
        <img alt="" itemprop="image" src="https://tma15.github.io/img/2020/03/15/lstm-lm.png"/>
      </div>
    </a>
  
</article>

</div>
<br>




            </div>
          </div>
          <div id="post-footer" class="post-footer main-content-wrap">
            
              
                
                
                  <div class="post-footer-tags">
                    <span class="text-color-light text-small">タグ</span><br/>
                    
  <a class="tag tag--primary tag--small" href="https://tma15.github.io/tags/nlp/">nlp</a>

  <a class="tag tag--primary tag--small" href="https://tma15.github.io/tags/neural_network/">neural_network</a>

  <a class="tag tag--primary tag--small" href="https://tma15.github.io/tags/paper/">paper</a>

                  </div>
                
              
            
            <div class="post-actions-wrap">
  
      <nav >
        <ul class="post-actions post-action-nav">
          
            <li class="post-action">
              
                <a class="post-action-btn btn btn--default tooltip--top" href="https://tma15.github.io/blog/2020/10/11/%E6%9B%B8%E8%A9%95%E3%82%B7%E3%83%AA%E3%82%B3%E3%83%B3%E3%83%90%E3%83%AC%E3%83%BC%E7%99%BA-%E3%82%B9%E3%82%AD%E3%83%AB%E3%81%AE%E6%8E%9B%E3%81%91%E7%AE%97%E3%81%A7%E5%B9%B4%E5%8F%8E%E3%81%8C%E5%A2%97%E3%81%88%E3%82%8B-%E8%A4%87%E6%A5%AD%E3%81%AE%E6%80%9D%E8%80%83%E6%B3%95/" data-tooltip="【書評】シリコンバレー発 スキルの掛け算で年収が増える 複業の思考法">
              
                  <i class="fa fa-angle-left"></i>
                  <span class="hide-xs hide-sm text-small icon-ml">次</span>
                </a>
            </li>
            <li class="post-action">
              
                <a class="post-action-btn btn btn--default tooltip--top" href="https://tma15.github.io/blog/2020/09/25/pythonyoutube-api%E3%81%8B%E3%82%89%E5%8B%95%E7%94%BB%E6%83%85%E5%A0%B1%E3%82%92%E5%8F%96%E5%BE%97/" data-tooltip="【Python】YouTube APIから動画情報を取得">
              
                  <span class="hide-xs hide-sm text-small icon-mr">前</span>
                  <i class="fa fa-angle-right"></i>
                </a>
            </li>
          
        </ul>
      </nav>
    <ul class="post-actions post-action-share" >
      
        <li class="post-action hide-lg hide-md hide-sm">
          <a class="post-action-btn btn btn--default btn-open-shareoptions" href="#btn-open-shareoptions">
            <i class="fa fa-share-alt"></i>
          </a>
        </li>
        
          <li class="post-action hide-xs">
            <a class="post-action-btn btn btn--default" target="new" href="https://twitter.com/intent/tweet?text=https://tma15.github.io/blog/2020/10/03/%E8%87%AA%E7%84%B6%E8%A8%80%E8%AA%9E%E5%87%A6%E7%90%86-%E3%81%82%E3%81%AA%E3%81%9F%E3%81%AEbert%E3%81%AB%E5%AF%BE%E3%81%99%E3%82%8Bfine-tuning%E3%81%AF%E3%81%AA%E3%81%9C%E5%A4%B1%E6%95%97%E3%81%99%E3%82%8B%E3%81%AE%E3%81%8B-%E8%AB%96%E6%96%87%E7%B4%B9%E4%BB%8B/">
              <i class="fa fa-twitter"></i>
            </a>
          </li>
        
      
      
      <li class="post-action">
        
          <a class="post-action-btn btn btn--default" href="#table-of-contents">
        
          <i class="fa fa-list"></i>
        </a>
      </li>
    </ul>
  
</div>

            
              
            
          </div>
        </article>
        <footer id="footer" class="main-content-wrap">
  <span class="copyrights">
    &copy; 2021 Takuya Makino. All Rights Reserved
  </span>
</footer>

      </div>
      <div id="bottom-bar" class="post-bottom-bar" data-behavior="1">
        <div class="post-actions-wrap">
  
      <nav >
        <ul class="post-actions post-action-nav">
          
            <li class="post-action">
              
                <a class="post-action-btn btn btn--default tooltip--top" href="https://tma15.github.io/blog/2020/10/11/%E6%9B%B8%E8%A9%95%E3%82%B7%E3%83%AA%E3%82%B3%E3%83%B3%E3%83%90%E3%83%AC%E3%83%BC%E7%99%BA-%E3%82%B9%E3%82%AD%E3%83%AB%E3%81%AE%E6%8E%9B%E3%81%91%E7%AE%97%E3%81%A7%E5%B9%B4%E5%8F%8E%E3%81%8C%E5%A2%97%E3%81%88%E3%82%8B-%E8%A4%87%E6%A5%AD%E3%81%AE%E6%80%9D%E8%80%83%E6%B3%95/" data-tooltip="【書評】シリコンバレー発 スキルの掛け算で年収が増える 複業の思考法">
              
                  <i class="fa fa-angle-left"></i>
                  <span class="hide-xs hide-sm text-small icon-ml">次</span>
                </a>
            </li>
            <li class="post-action">
              
                <a class="post-action-btn btn btn--default tooltip--top" href="https://tma15.github.io/blog/2020/09/25/pythonyoutube-api%E3%81%8B%E3%82%89%E5%8B%95%E7%94%BB%E6%83%85%E5%A0%B1%E3%82%92%E5%8F%96%E5%BE%97/" data-tooltip="【Python】YouTube APIから動画情報を取得">
              
                  <span class="hide-xs hide-sm text-small icon-mr">前</span>
                  <i class="fa fa-angle-right"></i>
                </a>
            </li>
          
        </ul>
      </nav>
    <ul class="post-actions post-action-share" >
      
        <li class="post-action hide-lg hide-md hide-sm">
          <a class="post-action-btn btn btn--default btn-open-shareoptions" href="#btn-open-shareoptions">
            <i class="fa fa-share-alt"></i>
          </a>
        </li>
        
          <li class="post-action hide-xs">
            <a class="post-action-btn btn btn--default" target="new" href="https://twitter.com/intent/tweet?text=https://tma15.github.io/blog/2020/10/03/%E8%87%AA%E7%84%B6%E8%A8%80%E8%AA%9E%E5%87%A6%E7%90%86-%E3%81%82%E3%81%AA%E3%81%9F%E3%81%AEbert%E3%81%AB%E5%AF%BE%E3%81%99%E3%82%8Bfine-tuning%E3%81%AF%E3%81%AA%E3%81%9C%E5%A4%B1%E6%95%97%E3%81%99%E3%82%8B%E3%81%AE%E3%81%8B-%E8%AB%96%E6%96%87%E7%B4%B9%E4%BB%8B/">
              <i class="fa fa-twitter"></i>
            </a>
          </li>
        
      
      
      <li class="post-action">
        
          <a class="post-action-btn btn btn--default" href="#table-of-contents">
        
          <i class="fa fa-list"></i>
        </a>
      </li>
    </ul>
  
</div>

      </div>
      <div id="share-options-bar" class="share-options-bar" data-behavior="1">
  <i id="btn-close-shareoptions" class="fa fa-close"></i>
  <ul class="share-options">
    
      <li class="share-option">
        <a class="share-option-btn" target="new" href="https://twitter.com/intent/tweet?text=https%3A%2F%2Ftma15.github.io%2Fblog%2F2020%2F10%2F03%2F%25E8%2587%25AA%25E7%2584%25B6%25E8%25A8%2580%25E8%25AA%259E%25E5%2587%25A6%25E7%2590%2586-%25E3%2581%2582%25E3%2581%25AA%25E3%2581%259F%25E3%2581%25AEbert%25E3%2581%25AB%25E5%25AF%25BE%25E3%2581%2599%25E3%2582%258Bfine-tuning%25E3%2581%25AF%25E3%2581%25AA%25E3%2581%259C%25E5%25A4%25B1%25E6%2595%2597%25E3%2581%2599%25E3%2582%258B%25E3%2581%25AE%25E3%2581%258B-%25E8%25AB%2596%25E6%2596%2587%25E7%25B4%25B9%25E4%25BB%258B%2F">
          <i class="fa fa-twitter"></i><span>Twitterで共有</span>
        </a>
      </li>
    
  </ul>
</div>
<div id="share-options-mask" class="share-options-mask"></div>
    </div>
    
    <div id="about">
  <div id="about-card">
    <div id="about-btn-close">
      <i class="fa fa-remove"></i>
    </div>
    
      <img id="about-card-picture" src="https://www.gravatar.com/avatar/dfef3d85434946d893b00f14fa3b80ed?s=110" alt="プロフィール画像" />
    
    <h4 id="about-card-name">Takuya Makino</h4>
    
      <div id="about-card-bio">自然言語処理の研究開発に従事しています。自然言語処理に関する研究から製品化に向けた開発に興味を持っています。本ブログでは自然言語処理、機械学習、Python、C++に関する記事が多めです。詳細は<a href="https://tma15.github.io/about/">プロフィール</a>を御覧ください。</div>
    
    
      <div id="about-card-job">
        <i class="fa fa-briefcase"></i>
        <br/>
        自然言語処理の研究開発に従事
      </div>
    
    
      <div id="about-card-location">
        <i class="fa fa-map-marker"></i>
        <br/>
        Kanagawa, Japan
      </div>
    
  </div>
</div>

    

    
  
    
      <div id="cover" style="background-image:url('https://tma15.github.io/images/cover.jpg');"></div>
    
  


    
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js" integrity="sha256-BbhdlvQf/xTY9gja0Dq3HiwQF8LaCRTXxZKRutelT44=" crossorigin="anonymous"></script>

  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js" integrity="sha256-/BfiIkHlHoVihZdc6TFuj7MmJ0TWcWsMXkeDFwhi0zw=" crossorigin="anonymous"></script>

<script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/2.1.7/js/jquery.fancybox.min.js" integrity="sha256-GEAnjcTqVP+vBp3SSc8bEDQqvWAZMiHyUSIorrWwH50=" crossorigin="anonymous"></script>


<script src="https://tma15.github.io/js/script-pcw6v3xilnxydl1vddzazdverrnn9ctynvnxgwho987mfyqkuylcb1nlt.min.js"></script>


<script lang="javascript">
window.onload = updateMinWidth;
window.onresize = updateMinWidth;
document.getElementById("sidebar").addEventListener("transitionend", updateMinWidth);
function updateMinWidth() {
  var sidebar = document.getElementById("sidebar");
  var main = document.getElementById("main");
  main.style.minWidth = "";
  var w1 = getComputedStyle(main).getPropertyValue("min-width");
  var w2 = getComputedStyle(sidebar).getPropertyValue("width");
  var w3 = getComputedStyle(sidebar).getPropertyValue("left");
  main.style.minWidth = `calc(${w1} - ${w2} - ${w3})`;
}
</script>

<script>
$(document).ready(function() {
  hljs.configure({ classPrefix: '', useBR: false });
  $('pre.code-highlight > code, pre > code').each(function(i, block) {
    if (!$(this).hasClass('codeblock')) {
      $(this).addClass('codeblock');
    }
    hljs.highlightBlock(block);
  });
});
</script>


  
    
  




    
  </body>
</html>

