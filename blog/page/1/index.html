<!doctype html>
<html lang="ja">
    <head>
        <link href="http://fonts.googleapis.com/css?family=Anton" rel="stylesheet" type="text/css">
        <link href='http://fonts.googleapis.com/css?family=Inconsolata' rel='stylesheet' type='text/css'>
        <link href='http://fonts.googleapis.com/css?family=Bubblegum+Sans' rel='stylesheet' type='text/css'>
        <title>Blog | Now is better than never.</title>
        <meta charset="utf-8" />
        <link href='http://fonts.googleapis.com/css?family=Permanent+Marker' rel='stylesheet' type='text/css'>
        <link rel="stylesheet" href="/media/css/font-awesome.css">
        <link rel="stylesheet" href="/media/css/style.css">
        <link rel="stylesheet" href="/media/css/pygments.css">
        <link rel="shortcut icon" href="/media/img/background.jpg">
        <script type="text/x-mathjax-config">
            MathJax.Hub.Config({
                tex2jax: { inlineMath: [['$','$'],['\\(','\\)']] }
            });
        </script>
        <script type='text/javascript' src='http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML'></script>
        <!--[if lt IE 9]>
        <script src="http://html5shiv.googlecode.com/svn/trunk/html5.js">
        </script>
        <![endif]-->
    </head>
    <body >
        <header>
            <hgroup>
              <h1>Now is better than never.</h1>
              <h3>Although never is often better than right now.</h3>
            </hgroup>
        </header>
        <nav>
                                <ul>
<li><a href="/">Home</a></li><li><a href="/about">About</a></li><li><a href="/blog">Blog</a></li><li><a href="/blog/archive.html">Archive</a></li></ul>
        </nav>
        <article id="content">
                    <h1><a href="/blog/2014/1/2013-2014.html" class="title">2013年と2014年</a></h1>

<h2>2013年の振り返り</h2>

<p><a href="http://tma15.github.io/blog/2013/1/start-of-2013.html">2013年の抱負</a>と照らしあわせて。</p>

<h3>飲み過ぎない</h3>

<p>12月は飲み過ぎた。次の日に食欲がなくなったりしてしまったり、ひどい時は2,3日胃がむかむかする感じだったので、反省。回数は多くても、一回あたりに飲む量を少し減らしたい。他の月は適度に楽しめたと思う。いつもお世話になっている美容師さん曰く、</p>

<p>「最初に飲む量を決めると良い。」</p>

<h3>反転してシュート</h3>

<p>あまり出来なかった。どうしても前を向いている後ろの選手にはたこうという気持ちが強すぎる。一旦「パスを出せ」と怒られるくらいのプレーが必要かも。</p>

<h3>論文を読む</h3>

<p>二日に一本読む、というのは出来なかった（無謀すぎた...）。とはいえ、すずかけ台論文読み会を主催し、今年は7回開催することが出来た点に関しては満足している。</p>

<p>この読み会は次の二つの（自分が享受したい）メリットを狙って、参加者は少人数にしぼり、基本的には参加者は読んだ論文をみんなの前で紹介するというスタイルにした:</p>

<ol>
<li>他の参加者にわかりやすく伝えるようにするためにより紹介する論文を読み込むようになる</li>
<li>参加者は少人数に絞っているので、気楽に場を止めて質問することができる</li>
</ol>

<p>ので自分を含め、参加した人の得るものはそれなりに多くできたと思う。ただ、自分はまだ曖昧な理解をしていることも多く、ありがたいご指摘も多々受けるので、もっと読み込まなければいけないところ。</p>

<p>参加者の規模はこのままで良いのだけど、紹介する論文を参加者にとって有益そうなものにする仕組みを採用すべきかな、とぼんやり考えたり。ある程度人数がいれば、対象となる学会を絞って、気になる論文に投票するという形もありだと思うけど、少人数ではこの形は難しいので、2014年は色々と模索したい。</p>

<h2>2014年の抱負</h2>

<p>いくつもあっても大変なので大きく二つ。</p>

<h3>論文を書く</h3>

<p>有名どころの国際学会の論文を読んでいるのは、自分もそういった国際学会に論文を通すため。査読付き国際学会に論文を通す、というのを目標にすべきところだけど、まずは論文を書く回数を増やす。あと、論文を書き始めるタイミングを、研究を始めるとき、あるいは研究の途中にする。論文を書き始めるタイミングは早いほうが良いという意見については色々なところで目にする気がする。例えば、</p>

<p><a href="http://www.cs.ucr.edu/~eamonn/Keogh_SIGKDD09_tutorial.pdf">How to do good research, get it published in SIGKDD and get it cited</a>とか</p>

<blockquote class="twitter-tweet" lang="ja"><p><a href="https://twitter.com/Pnnc205j">@Pnnc205j</a> だから早い段階からの論文執筆開始を推しているのさ。こうすると自分の研究の進め方につっこみが入れやすいしね。</p>&mdash; Tetsuya Sakai (酒井哲也) (@tetsuyasakai) <a href="https://twitter.com/tetsuyasakai/statuses/184170104878141440">2012, 3月 26</a></blockquote>

<script async="async" src="//platform.twitter.com/widgets.js" charset="utf-8"></script>

<p>とか。<a href="http://ymatsuo.com/japanese/ronbun_eng.html">松尾ぐみの論文の書き方：英語論文</a>でその重要性について書かれている、論文の「完成度を上げる」ためにも、早く書くことがとても大事なんだと思う。あとは、取り組むタスクに対して研究になりそうなところを目を凝らして物色したい。</p>

<h3>サボれないフットサル・サッカー環境に身を置く</h3>

<p>T村さんから指摘されたが、普段からヌルい空気でプレーしていることが攻守の切り替えの遅さを招いている気がする。仲間内でプレーしている時は大きな問題ではないけど、まじめな試合でこのことが致命的に自分の価値を下げていた。みんなで楽しむフットサル・サッカーも続けるけど、サボれない場所でもプレーする機会をつくりたいところ。とりあえずは個サルに参加して、知らない人の前で「情けないプレーなんてできない」と思える環境を少なくとも月に1回はつくる。</p>

<p><br />
<p class="date">2014-01-02</p></p>
<p><br><br>
<h1><a href="/blog/2013/12/mct.html" class="title">エッセイ"Towards the Machine Comprehension of Text"のメモ</a></h1>
<p><a href="http://research.microsoft.com/apps/pubs/default.aspx?id=206771">エッセイ</a>の一部をメモ。</p>
<p>主張をまとめると「自然言語の機械的な理解には、大規模なデータ、性能の良い機械学習も重要だけど、言語の構造をしっかり考えることも大事」。</p>
<h2>Introduction</h2>
<ul>
<li>Machine Comprehension of Text (MCT) (テキストの機械的理解) は人工知能のゴールである</li>
<li>このゴールを達成したかどうかを確かめるために、研究者はよくチューリングテストを思い浮かべるが、Levesque (2013)が指摘するように、これは機械を知的に向かわせる、というよりは人間の知能を下げるほうに作業者を差し向けてしまう<ul>
<li>※  チューリングテストとは、ある人間から見て、二人の対話のどちらが人間かどうか判別するテスト</li>
</ul>
</li>
<li>Levesqueはまた、チューリングテストよりも、世界知識を必要とするような選択肢が複数ある問題のほうが適しているとも主張している</li>
<li>このエッセイでは、MCTは、"ネイティブスピーカーの大半が正しく答えられる質問に対して機械が答えた回答が、ネイティブスピーカーが納得できるものであり、かつ関連していない情報を含んでいなければ、その機械はテキストを理解しているもの"とする (つまり質問応答)</li>
<li>このエッセイのゴールは、テキストの機械的理解という問題に何が必要なのかを観察することである</li>
</ul>
<h3>How To Measure Progress</h3>
<ul>
<li>複数の選択肢がある質問応答のデータセットをクラウドソーシングを利用して作った<ul>
<li>7歳の子供が読めるレベルのフィクションの短いストーリー</li>
</ul>
</li>
<li>Winograd Schema Test proposal (Levesque, 2013) は、質問と回答のペアは世界知識を要求するように注意深く設計されているので、生成には専門知識を要する質問を使うことを提案している<ul>
<li>"それは紙で出来ているので、ボールはテーブルから落ちた"の"それ"は何を指しているか？</li>
</ul>
</li>
<li>クラウドソーシングなのでスケーラビリティもある</li>
<li>進捗が早ければ、問題の難易度を上げることもできる<ul>
<li>語彙数を現状の8000から増やす</li>
<li>ノンフィクションなストーリーを混ぜる</li>
<li>タスクの定義を変える<ul>
<li>正解が1つ以上、あるいは正解が1つもない問題など</li>
<li>回答の根拠を出力するようにする</li>
</ul>
</li>
</ul>
</li>
<li>興味深いことは、ランダムな回答をするベースラインでは25%が正しい回答を得られる一方で、単純な単語ベースな手法が60%で、最近のモダンな含意認識システムを使っても60%くらいであることである</li>
</ul>
<h2>Desiderata and some Recent Work</h2>
<p>machine comprehensionに必要なものは、興味深い未解決な問題と通じている</p>
<ol>
<li>意味の表現は二つの意味でスケーラブルであるべきである、すなわち (1) 複数ソースのノイジーなデータから教師なし学習で学習できて、 (2) 任意のドメインの問題に適用できるべきである</li>
<li>モデルが巨大で複雑になっても、推論はリアリタイムでおこなえるべきである</li>
<li>構築、デバッグの簡易化のためにシステムはモジュール化すべきである<ul>
<li>モジュラ性はシステムを効率的に反応できるようにするべきである</li>
</ul>
</li>
<li>エラーが起きた時に、何故それが起きたか理解可能にするために、各モジュールは解釈可能であるべきであり、同様にモジュールの構成も解釈可能であるべきである</li>
<li>システムは単調的に修正可能であるべきである: 起きたエラーに対して、別のエラーを引き起こさずに、どのようにモデルを修正すればよいかが明白であるべきである</li>
<li>システムは意味表現に対して論理的推論をおこなえるべきである<ul>
<li>システムの入力のテキストの意味表現とシステムの世界モデルを組み合わせることで論理的な結論をだせるべきである</li>
<li>もろさを避けるため、また根拠を正しく結合するために、論理的思考は確率的であるべきなようである (Richardson and Domingos, 2006)</li>
</ul>
</li>
<li>システムは質問可能であるべきである<ul>
<li>任意の仮説に関して、真であるかどうか (の確率) を断言することができること</li>
<li>私達はなぜその断言ができるか理解することができるべきである</li>
</ul>
</li>
</ol>
<h3>最近の研究では</h3>
<ul>
<li>論理形式を文に対してタグ付けするなど、意味のモデル化はアノテーションコストがとても高い<ul>
<li>興味深い代替手段としては、質問-回答のペアから論理形式を帰納するアノテーションがより低いものがある (Liang et al., 2011)</li>
<li>教師なし学習でやる研究もある (Goldwassar et al. (2011) は60%の精度、ただし教師あり学習は80%)</li>
</ul>
</li>
<li>データはクラウドソーシングを利用すればスケールする (特にゲームとして提供すれば (Ahn and Dabbish, 2004))</li>
<li>大量のラベルなしデータを使えばある粒度の意味のモデル化はできる (らしい) (Mikolove et al., 2013)</li>
<li>意味モデル化のもう一つの問題は、あるタスク用に作ったモデルが他のタスクに使えないこと</li>
<li>とても難しいタスクに挑戦するとき、モジュラ性、デバッグ性、解釈性は、良い精度を出すのに役立つ<ul>
<li>画像分類タスクの現在のレコードホルダーが畳み込みネットワークが実際に何をしているのかを理解するための手法を設計したのは偶然の一致ではない: (Zeiler and Fergus, 2013)</li>
<li>修正可能性も強く関連している<ul>
<li>現在の機械学習モデルは、誤った例を正しく分類できるように修正するとき、別の例で新たな誤りをしないという保証がない</li>
</ul>
</li>
<li>質問可能性は別のデバッグツールである<ul>
<li>理解が簡単であるほど、そのモデルはうまくいく</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2>Seven Signposts</h2>
<h3>How to Incorporate Structure in Learning?</h3>
<ul>
<li>初期のAIはルールベース<ul>
<li>経験的で、もろく、スケールしない</li>
</ul>
</li>
<li>機械学習手法は構造データを扱えるように拡張されているが、主な方法は統計的なものである: 教師あり学習の基本的な設定では、<ul>
<li>データはある分布から生成されると仮定</li>
<li>モデル、コスト関数 (しばしば凸関数) 、ラベル付きデータが必要</li>
<li>ゴールは手元の訓練データのエラーを最小化すること (正則化は簡単のため考えない)</li>
</ul>
</li>
<li>構造は、コスト関数の探索時に構造の制約を入れることで考慮される<ul>
<li>言語はすごく構造的なので、機械学習のモデルを微調整して構造を考慮するよりも、最初からこの構造を認識しておくべきである</li>
</ul>
</li>
<li>言い換えれば機械学習は不確かさを扱う基本的な方法である<ul>
<li>もし、あまりに早く不確かさをモデル化することが、データの構造について我々が知っていることのほとんどを無視してしまうことにつながるなら、この誘惑には対抗しなければならない</li>
<li>そして、ほとんどの機械学習のアルゴリズムは、とてもシンプルなラベル (例えば二値ラベル) を使って、この上なく見事に不確かさをモデル化するように調整されている</li>
<li>確率的なグラフィカルモデルはモデルの構造の問題に取り組んでいるが、モデルの構造は人手で設計されているのでスケールしない</li>
</ul>
</li>
<li>先程述べたように、最近の研究では論理構造と統計モデルを直接組み合わせているが、まだスケーラビリティが問題ある</li>
<li>私達は、一つの極端 (人手で設計したルールに基づくAI) から、もう一つの極端 (明示的なルールがない機械学習) にきている</li>
</ul>
<h3>Do Large Data and Deep Learning Hold the Key?</h3>
<ul>
<li>ここ10年で、大量のデータを使うことで、昔から難しいタスクであった、質問応答、オントロジーの構築などですばらしい進歩が得られた<ul>
<li>deep neural networkがYouTubeの画像データを使って学習した</li>
</ul>
</li>
<li>しかしながら、意味のモデル化を避け、データの規模に頼っているシステムはもろい</li>
<li>AskMSR (人手で設計したルールに基づくQAシステム (Brill et al., 2002) に"How many feet are there in a lightyear? (1光年は何フィートか)？"という質問をしたら"Winnie the Pooh"と回答した<ul>
<li>ディズニーのキャラクターであるBuzz Lightyearが根拠になって回答された</li>
<li>意味の処理をちゃんとやっていない (質問は"how many"で始まっているので、回答は数字なはず)</li>
</ul>
</li>
<li>IBMのWatson (IRの様々な技術を組み合わせて人間のクイズ王に勝ったシステム) でさえもUSの都市に関する質問をしたらトロントと答えた</li>
<li>Deep Learningは強力なパラダイムである<ul>
<li>音声認識、画像分類では大きな成果を上げているが、まだシステムが解釈可能でなかったり、質問可能でなかったり、修正可能でなかったり、スケールしなかったりする</li>
</ul>
</li>
</ul>
<h3>Why is NLP so Hard?</h3>
<ul>
<li>自然言語処理はテキストの構造を直接モデル化する代替手段と見ることができるが、まだ初期段階である<ul>
<li>文が意味をなすかどうか、あるいは文が文法的かどうかという人間には簡単な問題すらまだ解けていない<ul>
<li>そのドメインにおけるリッチなモデルが、自然言語の理解には必要であるため<ul>
<li>しかし、リッチなモデルを作るには、自然言語処理の高い技術が必要</li>
<li>そのため、問題を限定して解くことが多い</li>
</ul>
</li>
<li>別の理由としては、NLPのタスクで機械学習のモデルを学習するときには、データの構造を直接利用する、というよりは二値ラベルのようなシンプルなものを利用するため問題をうまく解けないということも考えられる</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3>Can we Limit Scope for Manageability, yet Still Achive Scalability?</h3>
<ul>
<li>意味をモデル化する試みでは、簡単のために、よく問題を限定する</li>
<li>問題を限定すると、その解はスケールできない</li>
<li>問題を限定することには、科学的には意味があるが、私達は一般化が簡単な問題の限定、大規模データがある問題を探さなければならない</li>
</ul>
<h3>Are Brains Using Machine Learning?</h3>
<ul>
<li>あなたが友達が何か誤解をしていて苦労しているのに気づいたとしましょう</li>
<li>どうやって友達を救いますか？<ul>
<li>彼を何テラバイトもの訓練データとともに部屋に閉じ込め、「一週間これでパラメータを更新しといてね」ということはしないでしょう</li>
<li>あなたはあっという間にもっともありがちな彼の誤解が何なのかを考える<ul>
<li>「彼が誤解を修正すべきところはどこなんだろう」</li>
<li>あなたは一つや二つ、彼に質問をするかもしれない: 彼には質問することができる</li>
</ul>
</li>
</ul>
</li>
<li>つまり、あなたは彼の思考に関する解釈可能なモデルを持っていることになる</li>
<li>モジュラ性は人間の学習の強い区分けによって提案される<ul>
<li>人間は自転車に乗る方法を学ぶ時に歯の磨き方を忘れない</li>
<li>機械は、あらたに間違えたことを修正する時に、もともと正しく分類できていたものを間違えるようになってしまう</li>
</ul>
</li>
<li>難しいタスクにおいてさえ、人間は何かの意味を認識する時に、大量のラベル付きデータを使わない<ul>
<li>学習のプロセスは世界知識のモデルを更新する小さなステップに分割される<ul>
<li>見えない統計的なパラメータを更新しているわけではない</li>
</ul>
</li>
<li>人間は記録、更新が簡単な意味のモデルを持たなければならない<ul>
<li>少なくとも、彼らのモデルは解釈可能で、修正可能で、質問可能であることを示唆している</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2>Discussion</h2>
<ul>
<li>AIの初期からあるルールベースからあって、研究者はそれに依存し過ぎることに慎重であることは明白であった</li>
<li>ルールベースなシステムが解けない問題は、あまりない例外であり、それを解けるようにするために人手でルールを更新するというのは大規模なデータに対してスケールしない</li>
<li>機械学習は多くの場合、強力なツールであるのだけど、多くの場合解釈が難しく、ラベル付きデータ無しに改善することが難しい</li>
<li>機械学習は、適切な場面で使えば当然強力なツールである<ul>
<li>データの構造を最大限活用した後に、データに残っている不確かさのモデル化に使うことに制限することが考えられる</li>
</ul>
</li>
<li>人間にとってテキストが曖昧性を持たないという事実は、不確かさのモデル化は解くべき重要な問題ではないことを示唆している<ul>
<li>不確かさがモデル化されなければならない状況やラベルが極めて単純な状況において機械学習アルゴリズムの使用を抑えて、代わりにリッチな構造の、曖昧性のないテキストのために設計された他の手段を模索することは、機械学習が基づく数学的な基礎を放棄しているわけではない</li>
</ul>
</li>
</ul>
<br />
<p class="date">2013-12-27</p></p>
<p><br><br>
<h1><a href="/blog/2013/12/penguins-in-sweaters-or-serendipitous-entity-search-on-user-generated-content.html" class="title">Penguins in Sweaters, or Serendipitous Entity Search on User-generated Content (CIKM2013)メモ</a></h1>
<p><a href="http://labs.yahoo.com/publication/penguins-in-sweaters-or-serendipitous-entity-search-on-user-generated-content/">proceeding</a>
slide: <a href="http://www.slideshare.net/mounialalmas/penguins-in-sweaters-or-serendipitous-entity-search-on-usergenerated-content">slideshare</a></p>
<h2>まとめ</h2>
<p>CIKM 2013でBest paperを取った、著者が全員女性(<a href="http://labs.yahoo.com/news/ilaria-bordino-yelena-mejova-mounia-lalmas-awarded-best-paper-at-cikm-2013/">参考</a>)という、自分が今まで読んだ中でおそらく一番華やかな論文で、Yahoo Answersを知識源として、セレンディピティ (思ってもみなかったけど、クエリと関連していること) を感じる検索を提供する話。
何か新たな手法を提案した、というよりは、Yahoo Answersという知識源を使うことで、何か思ってもみなかったけど、面白い検索結果を提供できるんじゃないかな〜というアイディアを実際に試してみた、という感じだろうか。</p>
<p>以下、メモ。</p>
<h2>Why/when do penguins wear sweaters?</h2>
<ul>
<li>タスマニアで起きた原油漏れで体に油がついてしまったペンギンが、再び元の生活に戻れるようにするためのチャリティーソング (James GordonのSweaters for Penguins)<ul>
<li>羽毛に原油がつくことで断熱性が落ち、ペンギンが凍えてしまう</li>
<li>くちばしで羽毛に付いた原油を落とそうとすることで体を傷つけてしまう</li>
</ul>
</li>
</ul>
<h3>Serendipity</h3>
<p>役に立つんだけど、特に探していたわけではないもの。</p>
<h3>Entity Search</h3>
<p>この論文ではWikipediaとYahoo! Answersから抽出した、メタデータで情報を豊富にしたentityネットワークを基にentity-driven serendipitous search systemを作成する。</p>
<h2>この論文の焦点</h2>
<h3>WHAT</h3>
<p>ウェブコミュニティの知識源はどのようなentity間の関係を提供するのか？</p>
<h3>WHY</h3>
<p>そのような知識源がどのように面白く、セレンディピティなブラウジング経験に寄与するのか？</p>
<h2>データ</h2>
<h3>Yahoo! Answers</h3>
<ul>
<li>ごくわずかにまとめられた意見、ゴシップ、個人情報</li>
<li>観点が多様</li>
</ul>
<h3>Wikipedia</h3>
<ul>
<li>高品質の情報が整理されている</li>
<li>ニッチなトピックが豊富</li>
</ul>
<h2>Entity &amp; Relation Extraction</h2>
<h3>Entity: Wikipediaに記述されている概念</h3>
<p>1 テキストから表層形を識別し、
2 Wikipediaのentityと紐付けして、</p>
<ul>
<li>文脈依存</li>
<li>文脈非依存な素性<ul>
<li>click log</li>
</ul>
</li>
</ul>
<p>3 Wikipediaのentityを、テキストとの関連度順に基いてランキングする (aboutnessスコア(34)を使ってランキングする)</p>
<h3>Reationship: tf/idfベクトルのコサイン類似度</h3>
<p>entityが現れるドキュメントを結合したものがベクトルで表される</p>
<h2>Dataset Features (Metadata)</h2>
<h3>Sentiment</h3>
<ul>
<li>SentiStrengthを用いてpositive &amp; negativeのスコアを計算する<ul>
<li>インフォーマルな英語の極性判定でstate-of-the-art</li>
<li>このままだと文書レベルでの極性判定<ul>
<li>文単位でpositive、negativeのスコアを割り当てて、それぞれの平均が文書単位の極性のスコアになる</li>
</ul>
</li>
<li>文書は複数のentityを含むことが多いのでentity単位での極性はうまく測れない</li>
<li>まずentityのそれぞれ前後10単語のウィンドウに対して極性のスコアを計算する</li>
</ul>
</li>
<li>attitudeとsentimentalityをウィンドウに対して計算する[Kucuktunc'12]<ul>
<li>attitude: positiveもしくはnegativeに対する傾向</li>
<li>sentimentality: 極性の大きさ</li>
</ul>
</li>
<li>entityが現れるウィンドウのattitude, sentimentalityの平均値がentity単位の素性とする</li>
</ul>
<h3>Quality</h3>
<ul>
<li>可読性</li>
<li>Flesch Reading Ease score[14]<ul>
<li>スコアが高いほど理解するのが難しい</li>
<li>スコアが低いほど理解するのが易しい</li>
</ul>
</li>
<li>Fig1は二つのデータセットにおけるエンティティの可読性の分布</li>
</ul>
<h3>Topical Category</h3>
<ul>
<li>Yahoo Content Taxonomy<ul>
<li>Table 2</li>
<li>二つのデータ・セット中の概念体型は使わない<ul>
<li>整合性をとるため</li>
<li>二つのデータセットにおける実験結果を比較するため</li>
</ul>
</li>
</ul>
</li>
<li>US-Englishのニュース記事を使って訓練した分類器で文書分類する</li>
<li>entityレベルの素性にするため、そのentityが現れた文書に割り当てられたカテゴリのうち、頻度が高い上位3つのカテゴリを素性にする</li>
</ul>
<h2>Retrieval</h2>
<h3>Algorithm: Lazy Randomwalk with restart</h3>
<ul>
<li>self-loop probability: beta
他のノードへの伝搬を遅らせて、random walkの開始ノードの重要性をより高める<ul>
<li>先行研究にしたがって、beta = 0.9 [6, 12]</li>
</ul>
</li>
<li>follow one of the out-links with probability: 1 - beta
エッジの重みに比例してrandom walkする</li>
<li>random jumpの確率は0 (alpha = 0)<ul>
<li>random jumpすると結果が悪くなるため</li>
</ul>
</li>
<li>反復の終了条件<ul>
<li>前回とのノルムの差が10^-6以下、もしくは30回反復した</li>
</ul>
</li>
</ul>
<h4>scoring method</h4>
<ul>
<li>popularなentityはどこにでも上位にランクされるのでこれらをフィルタリング</li>
</ul>
<h3>Testbed</h3>
<ul>
<li>2010~2011にGoogle Zeitgeistで最も検索されたクエリの中から、Wikipedia、Yahoo! Answersの文書中に共に現れる上位50件のクエリ</li>
</ul>
<h3>Precision @5, MAP</h3>
<ul>
<li>Precision: 66.8% on WP, 72.4% on YA</li>
<li>MAP: 0.716 on WP, 0.762 on YA
ふたつのデータセットでの性能は同等であるものの、ランキングされるentityにはあまり重複がないため、二つのランキング結果を結合すると性能が上がる<ul>
<li>Fagin et al. [13]のrank aggregationを使う</li>
</ul>
</li>
</ul>
<h3>Annotator agreement</h3>
<ul>
<li>1つのクエリにつき、annotatorは3人<ul>
<li>クラウドソーシングしてるので、信頼出来ないannotatorは排除</li>
</ul>
</li>
<li>(overlap): 0.85%<ul>
<li>馴染みのないクエリはagreementが低い (Secosteroid, Sally Kern, ...)</li>
</ul>
</li>
</ul>
<h3>Average overlap in top 5</h3>
<ul>
<li>results: 12%<ul>
<li>= 0.6 entity/top 5 entities</li>
</ul>
</li>
</ul>
<h3>Error analysis</h3>
<p>提案手法の有効性はクエリのentityのすぐとなりのentityをあまり上に挙げないことによる</p>
<ul>
<li>例) Egyptに最も近い二つのentity<ul>
<li>British Pacific Fleet, FC Groningen (WP)</li>
<li>Spring, IGN (YA)</li>
<li>Springは"Arab Spring"と間違えて識別された可能性があるが、このSpringは提案手法があまりEgyptの近くをみないので下位にランクされる</li>
</ul>
</li>
</ul>
<p>とても似通ったentityばかりが上位にランクされてしまうこともある</p>
<ul>
<li>例) インフルエンザのウィルス名、Mac PowerBookのバージョン名で上位がうまる<ul>
<li>random walk時に密度が高いサブグラフにトラップされてしまうことで起きる</li>
</ul>
</li>
</ul>
<p>entityは関連していないentityが近くに来ることがある</p>
<ul>
<li>entity extractionの失敗</li>
<li>文脈類似度を測るときのノイズ</li>
<li>例) 同音異義語が強くつながってしまう<ul>
<li>意味的な素性を使わずに類似度を測ってしまう</li>
<li>(意味が違うなら文脈も違う気もする...)</li>
</ul>
</li>
</ul>
<h2>制約</h2>
<p>二つのデータセットが、serendipitous searchに何をもたらしてくれるのか調べる</p>
<ul>
<li>YAとWPにおける実験結果を比べる</li>
<li>どの素性が検索結果に影響するのか調べる<ul>
<li>このために、データセットからメタデータを抽出する</li>
<li>そして、sentimentality, quality, topical categoryの次元に対して、検索に制約をかける</li>
</ul>
</li>
</ul>
<p>制約をかけたネットワークと、制約をかけていないネットワークでの結果を比べる</p>
<h3>Topic</h3>
<p>Question: クエリに対してトピック的にコヒーレントなentityは良い結果をもたらすのか？
Constraint 1: entityは少なくとも1つ以上のクエリと同じトピックカテゴリに属さなければならない</p>
<h3>High/Low Sentimentalyty</h3>
<p>Question: より感情的な(感情的でない)entityは良い結果をもたらすのか？
Constraint 2(3): entityは中央値(0.6 for YP, 0 for WP)よりも高いsentimentalityでなければならない</p>
<h3>High/Low Readability</h3>
<p>Question: より読みやすい(読みにくい)entityは良い結果をもたらすのか？
Constraint 4(5): entityのreadabilityスコアは中央値(46 for YA, 41 for WP)でなければならない</p>
<h3>制約の結果</h3>
<ul>
<li>low-sentimentalityとlow-readabilityが負の影響を持っている</li>
</ul>
<h2>Serendipity</h2>
<ul>
<li>accuracy以外にも推薦エンジンの性能を測ることが重要である</li>
<li>serendipity = unexpectedness + relevance</li>
<li>baselineの結果に入っていない結果のrelの平均<ul>
<li>relはannotatorの判断（？）</li>
</ul>
</li>
<li>baseline<ul>
<li>Top: 2つの商用検索エンジンの検索結果のうち、最も上位5位の検索結果に現れる回数が多いentity</li>
<li>Top Nwp: TopからqueryのWikipediaの記事を除いたもの。WPに対するバイアスを避けるため</li>
<li>Rel: 2つの商用検索エンジンから提案される関連クエリから得られる結果のうち、頻度が高い上位5件のentity</li>
<li>Top+Rel: Top, Relの和集合</li>
</ul>
</li>
<li>Table 5: 各baselineに対する、各制約条件で計算されたserendipity<ul>
<li>topic-constrainedな条件ではすべてのbasline/datasetにおけるserendipityを上回っている</li>
<li>YAは常にWPを上回っている</li>
<li>COMが一番良いserendipityを出せる</li>
<li>括弧の中の値は各制約条件で提示されたすべての結果に対するunexpectedでrelaventな結果の割合<ul>
<li>baselineによって弾かれていたentityも含めたときの値</li>
</ul>
</li>
<li>これはだいたいserendipityと同じくらいの高さになっている<ul>
<li>つまり、最も強いbasline Rel+Topと比べた時でさえ、提案手法はすごい数のunexpectedでrelaventな結果を検索している</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2>User-Perceived Quality</h2>
<ul>
<li>主観的な評価になるので、結果に値を入れるのでは無く、色々な条件での実験結果を比較する</li>
<li>順序付きリストの要素のペアワイズで比較する (順番はランダムに決める)<ul>
<li>最初のほうが良かった</li>
<li>二つ目のほうが良かった</li>
<li>両方だめだった</li>
</ul>
</li>
<li>reference result rankingを各次元に対して構築する<ul>
<li>ランキング結果の集合の和集合(?)</li>
</ul>
</li>
<li>各ペアは3人のannotatorにより評価される<ul>
<li>ほとんど重複がない時に評価するのは非常に手間がかかる</li>
<li>適切なランクを推定するために、すべての順序リスト中の要素のペアから比較するペアをサンプリングする<ul>
<li>votingによってreference result rankingにおける順位を決める</li>
<li>http://www.cs.cmu.edu/~callan/Papers/ecir11-jarguello.pdf</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3>Labeling</h3>
<ol>
<li>どちらの結果がよりクエリと関連しているか？</li>
<li>そのクエリに興味を持つ人がいたら、その人はこの結果に興味を持つと思うか？</li>
<li>あなたがそのクエリに興味がないとしても、この結果はおもしろいか？</li>
<li>そのクエリについてなにか新しいことを学んだか？</li>
</ol>
<h3>Table 6: ランキングの集合とreference ranking間のKendall's tau-b</h3>
<ul>
<li>personal interest (Q3)とrelevance (Q1)の好みの割合の差を計算する時に、おもしろいけど必ずしもクエリと関連している必要のないentityを見つけた<ul>
<li>Oil Spill -&gt; Sweaters for Penguins</li>
<li>Robert Pattinson -&gt; Water for Elephants</li>
<li>Egypt -&gt; Ptolematic Kingdom</li>
</ul>
</li>
<li>専門的には似ているけど、面白くない例<ul>
<li>Egypt -&gt; Cairo Conference</li>
<li>Netflix -&gt; Blu-ray Disc</li>
</ul>
</li>
<li>YAはreference rankに似た結果を出せている</li>
<li>Topical categoryはreferece rankingとの類似度を高めている</li>
<li>Sentiment &amp; Readabilityもreferece rankingとの類似度を高めている</li>
</ul>
<br />
<p class="date">2013-12-14</p></p>
<p><br><br>
<h1><a href="/blog/2013/11/read-naive-bayes-in-scikit-learn.html" class="title">scikit-learnのソースコードリーディング（ナイーブベイズ分類）</a></h1>
<p>個人的にはプログラミングの勉強は写経が一番頭に入る気がする、ということで読んでいた。</p>
<h2>気になったところ</h2>
<p>データに正規分布を仮定したときのナイーブベイズ分類器について。
平均を(\mu)、分散を(\sigma^2)としたときの正規分布は</p>
<p>[
p(x;\mu, \sigma^2) = \frac{1}{\sqrt{2\pi \sigma^2}} {\exp{-\frac{(x-\mu)^2}{2\sigma^2}}}
]</p>
<p>これのlogをとると、
[
\begin{split}
\log p(x;\mu, \sigma^2) &amp;= \log {\frac{1}{\sqrt{2\pi \sigma^2}} {\exp{-\frac{(x-\mu)^2}{2\sigma^2}}}}\
&amp;= -\frac{1}{2}\log (2\pi \sigma^2) - \frac{(x-\mu)^2}{2\sigma^2}
\end{split}
]</p>
<p>ナイーブベイズ分類器の対数尤度関数は、データがK次元ベクトルで表現されていて、それがN個あるとすると、</p>
<p>[
\begin{split}
\log L(X, Y; \mu, \sigma) &amp;= \log(\prod_{n=1}^N p(\mathbf{x}<em>n, y_n))\
&amp; = \log(\prod</em>{n=1}^N p(y_n)p(\mathbf{x}<em>n|y_n))\
&amp; = \sum</em>{n=1}^N \log p(y_n) + \sum_{n=1}^N \log p(\mathbf{x}<em>n|y_n)\
&amp; = \sum</em>{n=1}^N \log p(y_n) + \sum_{n=1}^N \sum_{k=1}^K\log p(x_{nk}|y_n)\
&amp; = \sum_{n=1}^N \log p(y_n) + \sum_{n=1}^N \sum_{k=1}^K {-\frac{1}{2}\log (2\pi \sigma_{y_nk}^2) - \frac{(x_{nk}-\mu_{y_nk})^2}{2\sigma_{y_nk}^2}}
\end{split}
]</p>
<p>サンプル(\mathbf{x})に対して出力される予測ラベル(\hat{y})は</p>
<p>[
\begin{split}
\hat{y} &amp;= \mathop{\arg\,\max}\limits_y \log p(\mathbf{x}, y)\
&amp;= \mathop{\arg\,\max}\limits_y \log p(y)p(\mathbf{x}|y)\
&amp; = \mathop{\arg\,\max}\limits_y {\log p(y) + \sum_{k=1}^K {-\frac{1}{2}\log (2\pi \sigma_{yk}^2) - \frac{(x_k-\mu_{yk})^2}{2\sigma_{yk}^2}}}
\end{split}
]</p>
<p>対数尤度関数をnumpyに落とすと</p>
<div class="codebox"><figure class="code"><div class="highlight"><pre><span class="sd">&quot;&quot;&quot;</span><br /><span class="sd">sigma.shape = (n_classes, n_features)</span><br /><span class="sd">mu.shape = (n_classes, n_features)</span><br /><span class="sd">&quot;&quot;&quot;</span><br />&nbsp;<br /><span class="n">joint_log_likelihood</span> <span class="o">=</span> <span class="p">[]</span><br /><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="n">classes</span><span class="p">)):</span><br />    <span class="c"># 事前分布の対数</span><br />    <span class="n">log_prior</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">class_piror</span><span class="p">[</span><span class="n">i</span><span class="p">])</span><br />    <span class="c"># log p(x|y)の対数の初項</span><br />    <span class="n">log_gauss1</span> <span class="o">=</span> <span class="o">-</span><span class="mf">0.5</span> <span class="o"><em></span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">2</span> <span class="o"></em></span> <span class="n">np</span><span class="o">.</span><span class="n">pi</span> <span class="o"><em></span> <span class="n">sigma</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="p">:]))</span><br />    <span class="c"># log p(x|y)の対数の第二項</span><br />    <span class="n">log_gauss2</span> <span class="o">=</span> <span class="o">-</span><span class="mf">0.5</span> <span class="o"></em></span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">((</span><span class="n">X</span> <span class="o">-</span> <span class="n">mu</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="p">:])</span> <span class="o"><strong></span> <span class="mi">2</span> <span class="o">/</span> <span class="n">sigma</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="p">:])</span><br />    <span class="c"># クラスiの尤度のlogを取った値</span><br />    <span class="n">joint_log_likelihood</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">log_prior</span> <span class="o">+</span> <span class="n">log_gauss1</span> <span class="o">+</span> <span class="n">log_gauss2</span><span class="p">)</span><br /></pre></div><br /><figcaption>Python</figcaption></figure></div>
<p><br />
となる。と思っていた。ところがscikit-learnのGaussianNBの該当箇所を見て見ると、</p>
<div class="codebox"><figure class="code"><div class="highlight"><pre><span class="k">def</span> <span class="nf"><em>joint_log_likelihood</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span><br />        <span class="n">X</span> <span class="o">=</span> <span class="n">array2d</span><span class="p">(</span><span class="n">X</span><span class="p">)</span><br />        <span class="n">joint_log_likelihood</span> <span class="o">=</span> <span class="p">[]</span><br />        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">classes</em></span><span class="p">)):</span><br />            <span class="n">jointi</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">class_prior_</span><span class="p">[</span><span class="n">i</span><span class="p">])</span><br />            <span class="n">n_ij</span> <span class="o">=</span> <span class="o">-</span> <span class="mf">0.5</span> <span class="o"><em></span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">pi</span> <span class="o"></em></span> <span class="bp">self</span><span class="o">.</span><span class="n">sigma_</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="p">:]))</span> <span class="c"># np.piの前に2がない</span><br />            <span class="n">n_ij</span> <span class="o">-=</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(((</span><span class="n">X</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">theta_</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="p">:])</span> <span class="o"></strong></span> <span class="mi">2</span><span class="p">)</span> <span class="o">/</span><br />                                 <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">sigma_</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="p">:]),</span> <span class="mi">1</span><span class="p">)</span><br />            <span class="n">joint_log_likelihood</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">jointi</span> <span class="o">+</span> <span class="n">n_ij</span><span class="p">)</span><br /></pre></div><br /><figcaption>Python</figcaption></figure></div>
<p><br /></p>
<p>数式の展開が間違えているのだろうか...。それとも2は必要ないのだろうか...。</p>
<h2>参考</h2>
<ul>
<li><a href="http://r9y9.github.io/blog/2013/07/28/naive-bayes-formulation/">Naive Bayesの復習（導出編）</a></li>
<li><a href="http://cs.nyu.edu/~dsontag/courses/ml12/slides/lecture17.pdf">Naïve Bayes Lecture17</a></li>
</ul>
<br />
<p class="date">2013-11-10</p></p>
<p><br><br>
<h1><a href="/blog/2013/9/acl2013-summ-note.html" class="title">文書要約メモ（ACL2013）</a></h1>
<p><a href="http://aclweb.org/anthology//P/P13/">acl anthology</a>よりロングペーパーとして
採択された論文の中からSummarizationをタイトルに含む論文を探して概要だけを読んだときのメモ。</p>
<h1>Fast and Robust Compressive Summarization with Dual Decomposition and Multi-Task Learning (P13-1020.pdf)</h1>
<h2>概要</h2>
<ul>
<li>複数文書要約のための文選択、文圧縮を同時におこなうモデルを使った双対分解を提案。</li>
<li>先行研究のIneger Linear Programmingに基づいた手法と比べると<ul>
<li>提案手法はソルバーを必要としない</li>
<li>提案手法は有意に速い</li>
<li>提案手法は簡潔さ・情報の豊富さ・文法のきれいさが優れている</li>
</ul>
</li>
<li>さらに既存の抽出型要約、文圧縮の要約データを活用したマルチタスク学習を提案する</li>
<li>TAC2008のデータで実験をおこなって今までで一番高いROUGE値となった。</li>
</ul>
<h1>Using Supervised Bigram-based ILP for Extractive Summarization (P13-1099.pdf)</h1>
<h2>概要</h2>
<ul>
<li>Integer Linear Programmingによる抽出型文書要約において、bigramの重みを教師有り学習により推定する</li>
<li>regression modelによってbigramが参照要約の中でどれくらいの頻度で出現するかを推定。</li>
<li>学習では、参照要約中での真の頻度との距離が最小になるように学習をする</li>
<li>選択されるbigramの重みの総和が最大になるように文選択をおこなうような定式化をしている</li>
<li>提案手法は既存のILPな手法と比べてTACのデータにおいて良い性能であることと、TACのbestだったシステムとの比較結果を示す</li>
</ul>
<h1>Summarization Through Submodularity and Dispersion (P13-1100.pdf)</h1>
<h2>概要</h2>
<ul>
<li>Linらのサブモジュラな手法を一般化することにより新たな最適化手法を提案する</li>
<li>提案手法では要約にとって欲しい情報はサブモジュラ関数と非サブモジュラ関数の総和で表される。この関数をdispersionと呼ぶ</li>
<li>非サブモジュラ関数は要約の冗長性を除くために文同士の様々な似ていなさの度合いを図るために使う</li>
<li>三つのdispersion関数を使って、全部の場合で貪欲法を使っても最適解が得られることを示す</li>
<li>DUC 2004とニュース記事に対するユーザのコメントを使って実験</li>
<li>サブモジュラ関数だけを使ったモデルよりも良い性能であることを示す</li>
</ul>
<h1>Subtree Extractive Summarization via Submodular Maximization (P13-1101.pdf)</h1>
<h2>概要</h2>
<ul>
<li>@Pnnc205jさんの論文</li>
</ul>
<h1>Towards Robust Abstractive Multi-Document Summarization: A Caseframe Analysis of Centrality and Domain (P13-1121.pdf)</h1>
<h2>概要</h2>
<ul>
<li>文書要約において中心性とは元の文書の核となる部分を含むべきだということ</li>
<li>既存の手法は冗長性を除いたり文圧縮をおこなうことで中心性を得ようと試みている</li>
<li>この論文では元文書のドメインを活用することで文書要約が、抽象型要約に向けてどれくらいこのようなパラダイムから前進できるかを調査する</li>
<li>実験ではcaseframeという意味的なレベルで人手の要約とシステムの要約の近さを図る</li>
<li>提案手法は<ul>
<li>より抽象的で、文のまとめあげをおこなう</li>
<li>topicalなcaseframeを他のシステムほど含まない</li>
<li>元文書だけから再構築はできないけど、同じドメインの文書を加えればできる</li>
</ul>
</li>
<li>実験結果は、本質的な改善は中心性を最適化するための式を作ることよりも、ドメイン知識が必要であることを示唆している</li>
</ul>
<h1>A Sentence Compression Based Framework to Query-Focused Multi-Document Summarization (P13-1136.pdf)</h1>
<h2>概要</h2>
<ul>
<li>クエリ指向型複数文書要約のための文圧縮を使った手法を提案する</li>
<li>構文木に基づく文圧縮モデル</li>
<li>ビームサーチのデコーダを提案。効率的、高圧縮。</li>
<li>圧縮するためのスコア関数にどうやって言語的な特徴やクエリとの関連性を組み込むのかを示す</li>
<li>DUC 2006, DUC 2007のstate-of-the-artよりも有意によくなることを示す</li>
</ul>
<h1>Domain-Independent Abstract Generation for Focused Meeting Summarization (P13-1137.pdf)</h1>
<h2>概要</h2>
<ul>
<li>ドメイン知識を使わずに会議の対話ログの抽象型要約をおこなう</li>
<li>Multiple-Squence Alignmentという他のドメインにも使いまわせる抽象的な要約のテンプレートを使う</li>
<li>Overgenerate-and-Rankというものを候補の生成、ランキングに使うらしい</li>
</ul>
<br />
<p class="date">2013-09-30</p></p>
<p><br><br><div class="bottom_article_nav">
<div class="prev">&lt; <a href="/blog/page/2/">older</a></div>
</div>
<div class="center"><a href="/blog/archive.html">archive</a></div></p>
        <br>
                        </article>

                <script type="text/javascript">

  var _gaq = _gaq || [];
  _gaq.push(['_setAccount', 'UA-20414370-4']);
  _gaq.push(['_trackPageview']);

  (function() {
    var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
    ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
    var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
  })();

</script>    </body>
</html>