<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>golang on Now is better than never.</title>
    <link>http://tma15.github.io/tags/golang/</link>
    <description>Recent content in golang on Now is better than never.</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sun, 31 Jan 2016 19:17:31 +0900</lastBuildDate>
    
	<atom:link href="http://tma15.github.io/tags/golang/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>N-best解の探索</title>
      <link>http://tma15.github.io/blog/2016/01/31/</link>
      <pubDate>Sun, 31 Jan 2016 19:17:31 +0900</pubDate>
      
      <guid>http://tma15.github.io/blog/2016/01/31/</guid>
      <description>系列ラベリングなどで最適なパスを探索する方法はビタビアルゴリズムで効率的に求められる。 上位N個のパスを探索する方法はビタビアルゴリズムと、A*アルゴリズムで効率的に求められる。 日本語入力を支える技術　～変わり続けるコンピュータと言葉の世界 (WEB+DB PRESS plus) の説明が分かりやすい。理解するために実装してみた。
 ラティスの構造は以下のとおり。パスの重みは上記コードを参照。
#0 n1 --- n3 / \ / \ bos x eos \ / \ / #1 n2 --- n4  ビタビアルゴリズムを適用後、バックトラックで最適なパスを選んだ場合の解と、 ビタビアルゴリズムを適用後、A*アルゴリズムで後ろ向きに最適なパスを順に選んだ場合の1-best解が一致している。 2-best以降もラティスの情報から、あっていることを確認。
下記の###以降はこのエントリ用に付け足した文字列。
$go test Backtrack ### ビタビアルゴリズムで求めたパス 1 ### n2を通って、 0 ### n3を通る 1-best: 1 ### n2を通って、 1-best: 0 ### n3を通る 2-best: 1 ### n2を通って、 2-best: 1 ### n4を通る 3-best: 0 ### n1を通って、 3-best: 0 ### n3を通る 4-best: 0 ### n1を通って、 4-best: 1 ### n4を通る  goで優先度付きキューを実装するには、heap - The Go Programming LanguageのExample (PriorityQueue) が参考になる。</description>
    </item>
    
    <item>
      <title>並列での学習アルゴリズムの追加</title>
      <link>http://tma15.github.io/blog/2015/09/01/</link>
      <pubDate>Mon, 31 Aug 2015 20:03:45 +0900</pubDate>
      
      <guid>http://tma15.github.io/blog/2015/09/01/</guid>
      <description>拙作のgonlineに並列での学習もサポートするようにした。 分散環境での学習は手間がかかりそうだったので並列での学習のみとしている。 並列での学習にはIterative Parameter Mixture (pdf)を提供している。
シングルコアで学習するよりは速いんだけど、モデルの平均を取る時のボトルネックが大きくて、学習データの量がそれほど多くない場合はあまり効果がなさそう (以下の実験では人工的に学習データを増やしている)。CPU数を増やすと、平均を計算するコストが大きくなるので単純に学習が速くなるわけではない 。平均を取るときも、二分木にして並列化をしているが O(N)がO(log N)になるくらいなので、CPUの数が少なければ平均の計算がとても速くなるわけでもない。 CPUは、1.7 GHz Intel Core i5を利用して、4コア利用時の学習速度とシングルコア利用時の学習速度をと比較してみる。
$wc -l news20.scale 15935 news20.scale $touch news20.scale.big $for i in 1 2 3 4 5; do cat news20.scale &amp;gt;&amp;gt; news20.scale.big; done $wc -l news20.scale.big 79675 news20.scale.big $time ./gonline train -a arow -m model -i 10 -t ./news20.t.scale -withoutshuffle -p 4 -s ipm ./news20.scale.big ./gonline train -a arow -m model -i 10 -t ./news20.t.scale -withoutshuffle -p 272.</description>
    </item>
    
    <item>
      <title>オンライン学習の実装いろいろ</title>
      <link>http://tma15.github.io/blog/2015/07/17/</link>
      <pubDate>Fri, 17 Jul 2015 23:09:00 +0900</pubDate>
      
      <guid>http://tma15.github.io/blog/2015/07/17/</guid>
      <description>最近はNLPなデモをgolangで実装して人に見せることが多くなってきた。 その時に、さっと使える機械学習ライブラリが欲しかったので、勉強がてら実装した。 実装が簡単で学習が速いオンライン学習手法を実装した。
gonline
パーセプトロンから、Confidence WeightedやAROWまでを提供している。各アルゴリズムは多値分類が可能なように拡張している。 news20 を使って評価はしたのだけど こちらの論文 と比べると精度が低めになっているので、もしかしたら 実装が怪しいかもしれない (パラメータチューニングをしていないだけの問題かもしれない)。 SCWはいつか実装する。
golangらしく？github releaseでバイナリの配布もしている (今回初めてやってみた)。 これを使えば、とりあえず何も考えずに分類器を学習させて予測することができる。</description>
    </item>
    
    <item>
      <title>二つの集合に重複して現れる要素の数を数える</title>
      <link>http://tma15.github.io/blog/2014/11/count-elem-go/</link>
      <pubDate>Sat, 08 Nov 2014 00:00:00 +0000</pubDate>
      
      <guid>http://tma15.github.io/blog/2014/11/count-elem-go/</guid>
      <description>go言語で書いた (gist)。集合の要素は前もってソートしておいて、比較回数を減らしている。
package main import ( &amp;#34;fmt&amp;#34; &amp;#34;sort&amp;#34; &amp;#34;strconv&amp;#34; ) func CountDuplicateElem(x, y []string) int { i := 0 j := 0 num_match := 0 num_cmp := 0 for { if i &amp;gt;= len(x){ break } for { num_cmp += 1 if j &amp;gt;= len(y) { // 位置jがyの長さを超えたら終了  break } if x[i] &amp;lt; y[j] { // 辞書順でx[i]がy[j]よりも小さければ終了  break // ソートされていればjより大きな位置の文字で一致することは無い  } if x[i] == y[j] { num_match += 1 j += 1 break } j += 1 } i += 1 } return num_match } func NaiveCount(x, y []string) int { num_match := 0 for i := 0; i&amp;lt;len(x);i++{ for j := 0; j&amp;lt;len(y);j++{ if x[i] == y[j] { num_match += 1 } } } return num_match } func main() { k := []string{} l := []string{} for i:=0; i&amp;lt;100000; i++{ k = append(k, strconv.</description>
    </item>
    
    <item>
      <title>HMMを実装した</title>
      <link>http://tma15.github.io/blog/2014/10/hmm/</link>
      <pubDate>Sat, 25 Oct 2014 00:00:00 +0000</pubDate>
      
      <guid>http://tma15.github.io/blog/2014/10/hmm/</guid>
      <description>概要 勉強のためにGrahamさんが公開されている資料を参考に隠れマルコフモデルを実装した (このエントリでいう隠れマルコフモデルは、単語の品詞を推定するような教師あり学習)。 また、実験用のデータ、評価スクリプトも使用させて頂いている。
$./hmm train -i ../nlp-programming/data/wiki-en-train.norm_pos $./hmm test -i ../nlp-programming/data/wiki-en-test.norm &amp;gt; my_answer.pos $./nlp-programming/script/gradepos.pl ../nlp-programming/data/wiki-en-test.pos my_answer.pos Accuracy: 75.83% (3460/4563) Most common mistakes: NNS --&amp;gt; NN 49 RB --&amp;gt; NN 35 JJ --&amp;gt; DT 30 RB --&amp;gt; IN 29 NN --&amp;gt; JJ 28 NN --&amp;gt; IN 25 JJ --&amp;gt; NN 24 NN --&amp;gt; DT 24 NNP --&amp;gt; NN 22 VBN --&amp;gt; NN 22 特に工夫はしていないのでこんなものかという感じ。 コードはこちら。</description>
    </item>
    
    <item>
      <title>Goで日本語の文書を前処理して分類器を学習するところまでやってみる</title>
      <link>http://tma15.github.io/blog/2014/10/document-classification/</link>
      <pubDate>Mon, 20 Oct 2014 00:00:00 +0000</pubDate>
      
      <guid>http://tma15.github.io/blog/2014/10/document-classification/</guid>
      <description>概要 日本語の文書を単純な方法で分類器を学習するところまでの一連の処理をGoでやってみる。 分類器は何でも良いのだけど、先日書いたAdaGrad+RDAを使う。
ラベルが付いた日本語のデータがあるという前提で、以下の流れで進める。
 文書を文に分割する。今回は「。」で区切る。 文を形態素解析して名詞や動詞(表層形)を取り出し、文書をある単語を含む、含まないの二値で表現した素性ベクトルに変換する。 訓練データを使って分類器を学習して、できたモデルの中身を見てみる。  データ 下記URLから得られるテキストの一部を使って、ラベルをそれぞれ、「スポーツ」、「政治」、「Go言語」とラベルを付与し、第一カラムをラベル、第二カラムを文書としたCSVに保存しておく。
 本田圭佑:セリエＡ日本人４人目マルチ!惨敗ブラジル戦憂さ晴らし 観劇収支ズレどう説明、公私混同疑いも…小渕氏 古いプログラミング言語がなくならない理由  $cat data.csv スポーツ,ＡＣミランＦＷ本田圭佑（２８）が１９日のアウェー、ベローナ戦で... 政治,渕経済産業相が関連する政治団体の資金処理問題で、最も不透明と指摘されて... Go言語,編集者とこの本を5000部売れたらなという話をしたのをなんとなく覚えている。... &amp;hellip;以降は省略している。
ソースコード  mecab.go (gist) text.go (gist)  動かしてみる $./text data.csv &amp;gt; data $cat data スポーツ ２:1.000000 スルー:1.000000 本田:1.000000 セリエＡ:1.000000 アルゼンチン:1.000000... 政治 円:1.000000 なる:1.000000 者:1.000000 向け:1.000000 会:1.000000 収支:1.000000... Go言語 処理:1.000000 ため:1.000000 Go:1.000000 編集:1.000000 5000:1.000000... &amp;hellip;以降は省略している。これで、dataファイルに素性ベクトルが書き込まれる。 次に分類器を学習する。
$./adagrad -f data -m learn -w model できあがったモデルの中身を見てみる。
$cat model|grep &amp;#34;^スポーツ&amp;#34;|sort -k3 -nr|head スポーツ カルロス・テベス 0.</description>
    </item>
    
    <item>
      <title>AdaGrad&#43;RDAをGoで書いた</title>
      <link>http://tma15.github.io/blog/2014/10/go-adagrad/</link>
      <pubDate>Sat, 18 Oct 2014 00:00:00 +0000</pubDate>
      
      <guid>http://tma15.github.io/blog/2014/10/go-adagrad/</guid>
      <description>論文はこちら。
Adaptive Subgradient Methods for Online Learning and Stochastic Optimization
ソースコードはこちら。 多値分類問題にも対応できるようにした。二値分類問題と比べてヒンジ損失が少し変わる(ので重みの更新も二値分類の場合とと少し違う)。
データを次のように作成。
$perl -MList::Util=shuffle -e &amp;#39;print shuffle(&amp;lt;&amp;gt;)&amp;#39; &amp;lt; ../data/news20.binary &amp;gt; news $head -15000 news &amp;gt; news.train $tail -4996 news &amp;gt; news.test 例えばこのデータは素性の値が0.04くらいなので、その平均を取ると0.01よりも小さくなるため、式(24)中の右辺の第三項が0になり、ほとんどすべての重みが0になってしまう。 正則化項の重み&amp;copy;をもう少し小さくしてやると、次の結果になった(本当は論文のように交差検定をして決めてやったほうが良いけど、人手でチューニング)。
$./adagrad -f news.train -m learn -w model -l 1 -c 0.01 $./adagrad -f news.test -m test -w model -l 1 -c 0.01 Recall[-1]: 0.011142 (28/2513) Prec[-1]: 0.848485 (28/33) -- Recall[+1]: 0.997986 (2478/2483) Prec[+1]: 0.499295 (2478/4963) -- Acc: 0.</description>
    </item>
    
    <item>
      <title>PA-IIをGoで書いた</title>
      <link>http://tma15.github.io/blog/2014/10/go-pa2/</link>
      <pubDate>Sat, 18 Oct 2014 00:00:00 +0000</pubDate>
      
      <guid>http://tma15.github.io/blog/2014/10/go-pa2/</guid>
      <description>論文はこちら。
Online Passive-Aggressive Algorithms
ソースコードはこちら。 下の関数でおこなわれている重みの更新以外はほとんどパーセプトロンと一緒です。
func (p *PassiveAggressive) Update(X map[string]float64, y string, sign float64) Weight { loss := math.Max(0, 1-sign*Dot(X, p.weight[y])) // tau := loss / Norm(X) // PA  tau := loss / (Norm(X) + 1 / (2 * p.C)) // PA-II  if _, ok := p.weight[y]; ok == false { p.weight[y] = map[string]float64{} } for f, _ := range X { if _, ok := p.weight[y][f]; ok { p.</description>
    </item>
    
    <item>
      <title>パーセプトロンをGoで書いた</title>
      <link>http://tma15.github.io/blog/2014/10/go-perceptron/</link>
      <pubDate>Sat, 11 Oct 2014 00:00:00 +0000</pubDate>
      
      <guid>http://tma15.github.io/blog/2014/10/go-perceptron/</guid>
      <description>流行りに乗り遅れてGo言語始めました。ので、試しにパーセプトロンを書いてみました。 ソースコードはこちら。
素性ベクトルのフォーマットは&amp;lt;数値&amp;gt;:&amp;lt;数値&amp;gt; である必要はなくて、&amp;lt;文字列&amp;gt;:&amp;lt;数値&amp;gt; でも読み込めるようにしました。 また、ラベルの値も数値である必要はなくて、例えば以下のように「food」とか、「sports」というラベルも扱えるようにしています。 多値分類もできます。
sports soccer:1 baseball:1 food beef:1 pork:1 今回はLIBSVM Data: Classification, Regression, and Multi-labelで公開されている二値分類用データを使って動かしてみました。
$go build $./perceptron -f=../data/a1a -m=learn -w=model -l=10 $./perceptron -f=../data/a1a.t -m=test -w=model Acc: 0.8257203773097299 -fオプションで素性ベクトルのファイルを指定して、-mオプションで学習(learn)、テスト(test)のどちらかを指定して-lオプションでループ回数(デフォルトは10)を指定して、-wオプションで学習結果を保存するファイルを指定します。 テストする時は、-mオプションでtestを指定して、-fオプションでテストデータを指定してやれば予測します。-vオプションをつけると、各事例に対する予測ラベルを出力します。
このデータは、
$grep &amp;#34;^+1&amp;#34; ../data/a1a.t|wc -l 7446 $grep &amp;#34;^-1&amp;#34; ../data/a1a.t|wc -l 23510 とラベルの偏りがあり、すべての事例のラベルを-1と答えたらaccuracyは0.76程度なので、一応学習できているようです。
confusion matrixを書く元気は残っていなかったのでaccuracyしか出力しません・・・。 出力するようにしました。 (2014/09/17追記)</description>
    </item>
    
  </channel>
</rss>